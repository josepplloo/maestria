@book{Pohl2005,
	author = {Pohl, Klaus and B{\"{o}}ckle, G{\"{u}}nter and van der Linden, Frank},
	booktitle = {Springer-Verlag Berlin Heidelberg},
	doi = {10.1007/3-540-28901-1},
	file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Pohl, B{\"{o}}ckle, van der Linden - 2005 - Software Product Line Engineering Foundations, Principles and Techniques.pdf:pdf},
	isbn = {3-540-24372-0},
	mendeley-groups = {estado del arte},
	title = {{Software Product Line Engineering: Foundations, Principles and Techniques}},
	year = {2005}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}
@article{Rolland1998,
abstract = {The requirements engineering, information systems and software engineering communities recently advocated scenario-based approaches which emphasise the user/system interaction perspective in developing computer systems. Use of examples, scenes, narrative descriptions of contexts, mock-ups and prototypes-all these ideas can be called scenario-based approaches, although exact definitions are not easy beyond stating that these approaches emphasise some description of the real world. Experience seems to tell us that people react to ‘real things' and that this helps in clarifying requirements. Indeed, the widespread acceptance of prototyping in system development points to the effectiveness of scenario-based approaches. However, we have little understanding about how scenarios should be constructed, little hard evidence about their effectiveness and even less idea about why they work. The paper is an attempt to explore some of the issues underlying scenario-based approaches in requirements engineering and to propose a framework for their classification. The framework is a four-dimensional framework which advocates that a scenario-based approach can be well defined by itsform, content, purpose andlife cycle. Every dimension is itself multifaceted and a metric is associated with each facet. Motivations for developing the framework are threefold: (a) to help in understanding and clarifying existing scenario-based approaches; (b) to situate the industrial practice of scenarios; and (c) to assist researchers develop more innovative scenario-based approaches.},
author = {Rolland, C. and {Ben Achour}, C. and Cauvet, C. and Ralyt{\'{e}}, J. and Sutcliffe, a. and Maiden, N. and Jarke, M. and Haumer, P. and Pohl, K. and Dubois, E. and Heymans, P.},
doi = {10.1007/BF02802919},
file = {:Users/josepplloo/Documents/primer inmersion/REJoural.pdf:pdf},
isbn = {0947-3602},
issn = {0947-3602},
journal = {Requirements Engineering},
mendeley-groups = {estado del arte,informe},
number = {1},
pages = {23--47},
title = {{A proposal for a scenario classification framework}},
volume = {3},
year = {1998}
}
@article{Wieringa2006,
abstract = {The article presents the outcome of the discussions by members of the steering committee of the International Electrical and Electronic Engineering Requirements Engineering (RE) Conference, regarding paper classification and evaluation criteria for RE papers. Section two of the article sketches the rationale for the classification. Section three presents the classification, and section four concludes with a discussion of background ideas and related work.},
author = {Wieringa, Roel and Maiden, Neil and Mead, Nancy and Rolland, Colette},
doi = {10.1007/s00766-005-0021-6},
isbn = {0947-3602},
issn = {09473602},
journal = {Requirements Engineering},
keywords = {Paper classification,Paper evaluation criteria,Requirements engineering research,Research methods},
mendeley-groups = {informe},
number = {1},
pages = {102--107},
title = {{Requirements engineering paper classification and evaluation criteria: A proposal and a discussion}},
volume = {11},
year = {2006}
}

@inproceedings{Wohlin2014,
address = {New York, New York, USA},
author = {Wohlin, Claes},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering - EASE '14},
doi = {10.1145/2601248.2601268},
file = {:Users/josepplloo/Downloads/Presentaci{\'{o}}n SMS-1/Presentaci¢n SMS/Referencias/wohlin.pdf:pdf},
isbn = {9781450324762},
keywords = {replication,snowball search,snowballing,systematic,systematic literature review},
mendeley-groups = {estado del arte},
pages = {1--10},
publisher = {ACM Press},
title = {{Guidelines for snowballing in systematic literature studies and a replication in software engineering}},
url = {http://dl.acm.org/citation.cfm?doid=2601248.2601268},
year = {2014}
}

@article{Zhang2011,
abstract = {Context: Systematic literature review (SLR) has become an important research methodology in software engineering since the introduction of evidence-based software engineering (EBSE) in 2004. One critical step in applying this methodology is to design and execute appropriate and effective search strategy. This is a time-consuming and error-prone step, which needs to be carefully planned and implemented. There is an apparent need for a systematic approach to designing, executing, and evaluating a suitable search strategy for optimally retrieving the target literature from digital libraries. Objective: The main objective of the research reported in this paper is to improve the search step of undertaking SLRs in software engineering (SE) by devising and evaluating systematic and practical approaches to identifying relevant studies in SE. Method: We have systematically selected and analytically studied a large number of papers (SLRs) to understand the state-of-the-practice of search strategies in EBSE. Having identified the limitations of the current ad-hoc nature of search strategies used by SE researchers for SLRs, we have devised a systematic and evidence-based approach to developing and executing optimal search strategies in SLRs. The proposed approach incorporates the concept of 'quasi-gold standard' (QGS), which consists of collection of known studies, and corresponding 'quasi-sensitivity' into the search process for evaluating search performance. Results: We conducted two participant-observer case studies to demonstrate and evaluate the adoption of the proposed QGS-based systematic search approach in support of SLRs in SE research. Conclusion: We report their findings based on the case studies that the approach is able to improve the rigor of search process in an SLR, as well as it can serve as a supplement to the guidelines for SLRs in EBSE. We plan to further evaluate the proposed approach using a series of case studies on varying research topics in SE. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Zhang, He and Babar, Muhammad Ali and Tell, Paolo},
doi = {10.1016/j.infsof.2010.12.010},
file = {:Users/josepplloo/Downloads/Presentaci{\'{o}}n SMS-1/Presentaci¢n SMS/Referencias/Zhang, Babar, Tell - 2011 - Identifying relevant studies in software engineering.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Evidence-based software engineering,Quasi-gold standard,Search strategy,Systematic literature review},
mendeley-groups = {estado del arte,informe},
month = {jun},
number = {6},
pages = {625--637},
publisher = {Elsevier B.V.},
title = {{Identifying relevant studies in software engineering}},
url = {http://dx.doi.org/10.1016/j.infsof.2010.12.010 http://linkinghub.elsevier.com/retrieve/pii/S0950584910002260},
volume = {53},
year = {2011}
}

@inproceedings{Thum2016,
address = {New York, New York, USA},
author = {Th{\"{u}}m, Thomas and Winkelmann, Tim and Schr{\"{o}}ter, Reimar and Hentschel, Martin and Kr{\"{u}}ger, Stefan},
booktitle = {Proceedings of the Tenth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '16},
doi = {10.1145/2866614.2866628},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/10.1145@2866614.2866628.pdf:pdf},
isbn = {9781450340199},
keywords = {Multi product line,deductive verification,method contracts,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
pages = {97--104},
publisher = {ACM Press},
title = {{Variability Hiding in Contracts for Dependent Software Product Lines}},
url = {http://doi.acm.org/10.1145/2866614.2866628 http://dl.acm.org/citation.cfm?doid=2866614.2866628},
year = {2016}
}
@article{Petersen2008a,
archivePrefix = {arXiv},
arxivId = {2227123},
author = {Petersen, Kai and Vakkalanka, Sairam and Kuzniarz, Ludwik},
doi = {10.1016/j.infsof.2015.03.007},
eprint = {2227123},
isbn = {0-7695-2555-5},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {evidence based software engineering,systematic mapping studies,systematic reviews},
mendeley-groups = {propuesta,informe},
month = {aug},
pages = {1--18},
title = {{Guidelines for conducting systematic mapping studies in software engineering: An update}},
url = {http://dl.acm.org/citation.cfm?id=2227123 http://linkinghub.elsevier.com/retrieve/pii/S0950584915000646},
volume = {64},
year = {2015}
}

@article{Petersen2015,
abstract = {Context Systematic mapping studies are used to structure a research area, while systematic reviews are focused on gathering and synthesizing evidence. The most recent guidelines for systematic mapping are from 2008. Since that time, many suggestions have been made of how to improve systematic literature reviews (SLRs). There is a need to evaluate how researchers conduct the process of systematic mapping and identify how the guidelines should be updated based on the lessons learned from the existing systematic maps and SLR guidelines. Objective To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly. Method We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to defining the search and to conduct a quality assessment). Results In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given. Conclusion The most frequently followed guidelines are not sufficient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing findings.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Petersen, Kai and Vakkalanka, Sairam and Kuzniarz, Ludwik},
doi = {10.1016/j.infsof.2015.03.007},
eprint = {arXiv:1011.1669v3},
file = {:Users/josepplloo/Downloads/petersen2015.pdf:pdf},
isbn = {0360-1315},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Guidelines,Software engineering,Systematic mapping studies},
mendeley-groups = {estado del arte,propuesta},
month = {aug},
pages = {1--18},
pmid = {25246403},
publisher = {Elsevier B.V.},
title = {{Guidelines for conducting systematic mapping studies in software engineering: An update}},
url = {http://dx.doi.org/10.1016/j.infsof.2015.03.007 http://linkinghub.elsevier.com/retrieve/pii/S0950584915000646},
volume = {64},
year = {2015}
}

@article{Perovsek2015a,
abstract = {Inductive Logic Programming (ILP) and Relational Data Mining (RDM) address the task of inducing models or patterns from multi-relational data. One of the established approaches to RDM is propositionalization, characterized by transforming a relational database into a single-table representation. This paper presents a propositionalization technique called wordification which can be seen as a transformation of a relational database into a corpus of text documents. Wordification constructs simple, easy to understand features, acting as words in the transformed Bag-Of-Words representation. This paper presents the wordification methodology, together with an experimental comparison of several propositionalization approaches on seven relational datasets. The main advantages of the approach are: simple implementation, accuracy comparable to competitive methods, and greater scalability, as it performs several times faster on all experimental databases. Furthermore, the wordification methodology and the evaluation procedure are implemented as executable workflows in the web-based data mining platform ClowdFlows. The implemented workflows include also several other ILP and RDM algorithms, as well as the utility components that were added to the platform to enable access to these techniques to a wider research audience.},
author = {Perov{\v{s}}ek, Matic and Vavpeti{\v{c}}, An{\v{z}}e and Kranjc, Janez and Cestnik, Bojan and Lavra{\v{c}}, Nada},
doi = {10.1016/j.eswa.2015.04.017},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Perov{\v{s}}ek et al. - 2015 - Wordification Propositionalization by unfolding relational data into bags of words.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classification,Inductive Logic Programming,Propositionalization,Relational Data Mining,Text mining,Wordification,purpura,rojo},
mendeley-groups = {con doi,informe},
mendeley-tags = {purpura,rojo},
month = {oct},
number = {17-18},
pages = {6442--6456},
title = {{Wordification: Propositionalization by unfolding relational data into bags of words}},
url = {http://www.sciencedirect.com/science/article/pii/S095741741500247X http://linkinghub.elsevier.com/retrieve/pii/S095741741500247X},
volume = {42},
year = {2015}
}
@inproceedings{Guillaume2015,
abstract = {Feature modeling is a widely used formalism to characterize a set of products (also called configurations). $\backslash$r$\backslash$nAs a manual elaboration is a long and arduous task, numerous techniques have been proposed to reverse engineer feature models from various kinds of artefacts. But none of them synthesize feature attributes (or constraints over attributes) despite the practical relevance of attributes for documenting the different values across a range of products.$\backslash$r$\backslash$nIn this report, we develop an algorithm for synthesizing attributed feature models given a set of product descriptions. $\backslash$r$\backslash$nWe present sound, complete, and parametrizable techniques for computing all possible hierarchies, feature groups, placements of feature attributes, domain values, and constraints. $\backslash$r$\backslash$nWe perform a complexity analysis w.r.t. number of features, attributes, configurations, and domain size. We also evaluate the scalability of our synthesis procedure using randomized configuration matrices. $\backslash$r$\backslash$nThis report is a first step that aims to describe the foundations for synthesizing attributed feature models.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1502.04645},
author = {B{\'{e}}can, Guillaume and Behjati, Razieh and Gotlieb, Arnaud and Acher, Mathieu},
booktitle = {Proceedings of the 19th International Conference on Software Product Line - SPLC '15},
doi = {10.1145/2791060.2791068},
eprint = {1502.04645},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/dspl20015.pdf:pdf},
isbn = {9781450336130},
keywords = {attributed feature models,gris,naranja,product descriptions},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris,naranja},
pages = {1--10},
publisher = {ACM Press},
title = {{Synthesis of attributed feature models from product descriptions}},
url = {http://dl.acm.org/citation.cfm?doid=2791060.2791068},
year = {2015}
}
@article{Maldonado2014,
abstract = {Feature selection and classification of imbalanced data sets are two of the most interesting machine learning challenges, attracting a growing attention from both, industry and academia. Feature selection addresses the dimensionality reduction problem by determining a subset of available features to build a good model for classification or prediction, while the class-imbalance problem arises when the class distribution is too skewed. Both issues have been independently studied in the literature, and a plethora of methods to address high dimensionality as well as class-imbalance has been proposed. The aim of this work is to simultaneously explore both issues, proposing a family of methods that select those attributes that are relevant for the identification of the target class in binary classification. We propose a backward elimination approach based on successive holdout steps, whose contribution measure is based on a balanced loss function obtained on an independent subset. Our experiments are based on six highly imbalanced microarray data sets, comparing our methods with well-known feature selection techniques, and obtaining a better prediction with consistently fewer relevant features.},
author = {Maldonado, Sebasti{\'{a}}n and Weber, Richard and Famili, Fazel},
doi = {10.1016/j.ins.2014.07.015},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Maldonado, Weber, Famili - 2014 - Feature selection for high-dimensional class-imbalanced data sets using Support Vector Machines.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Data mining,Dimensionality reduction,Feature selection,Imbalanced data set,Support Vector Machine,azul,naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {azul,naranja},
month = {dec},
pages = {228--246},
title = {{Feature selection for high-dimensional class-imbalanced data sets using Support Vector Machines}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025514007154 http://linkinghub.elsevier.com/retrieve/pii/S0020025514007154},
volume = {286},
year = {2014}
}
@incollection{Antonio2009,
abstract = {Feature modeling is an important technique to capture commonalities and variabilities in a software product line ({\{}SPL).{\}} However, this kind of models shows a specific perspective, which is not sufficient to express all the characteristics and constraints of an {\{}SPL.{\}} Using a goal-oriented approach, such as i*, to complement (and help define) feature models would improve such models enhancing meaning and justification to features. Goal-oriented modelling provides a way to identify variabilities at an early phase of requirements, allowing alternative options to satisfy stakeholder's goals. The aim of this work is to benefit software product lines from the framework i*, a more expressive approach to requirements engineering of {\{}SPLs.{\}}},
author = {Ant{\'{o}}nio, Sandra and Ara{\'{u}}jo, Jo{\~{a}}o and Silva, Carla},
booktitle = {Proceedings of the {\{}ER{\}} 2009 Workshops ({\{}CoMoL{\}}, {\{}ETheCoM{\}}, {\{}FP-UML{\}}, {\{}MOST-ONISW{\}}, {\{}QoIS{\}}, {\{}RIGiM{\}}, {\{}SeCoGIS){\}} on Advances in Conceptual Modeling - Challenging Perspectives},
doi = {10.1007/978-3-642-04947-7_34},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/springer/10.1007@978-3-642-04947-734.pdf:pdf},
isbn = {978-3-642-04946-0},
keywords = {Goal-oriented approach,feature model,i* Framework,requirements engineering,rojo,software product line},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
pages = {286--295},
title = {{Adapting the i* Framework for Software Product Lines}},
url = {http://dx.doi.org/10.1007/978-3-642-04947-7{\_}34 http://link.springer.com/10.1007/978-3-642-04947-7{\_}34},
year = {2009}
}
@article{Wang2014a,
abstract = {Data mining can extract useful information from large databases. This paper presents the evolution of the intellectual structure in tourism destination literature as determined by means of bibliometric and social network analysis of 17 552 citations of 414 articles published in Social Sciences Citation Index and Sciences Citation Index journals from 1955 to 2011. This study found that tourism destination research is organized into four different concentrations of interest: destination image, tourist experience and stakeholder involvement, structural equation modeling, and customer relationship management. Future tourism destination research will probably continue to focus on these topics. This study presents a new way for researchers to profile development patterns objectively and provides a key reading method for searching useful research directions. Copyright {\textcopyright} 2014 by ASTM International.},
author = {Wang, Cheng-Hua and Chen, Shiu-Chun},
doi = {10.1520/JTE20120285},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/ASTM/JTE20120285.32209.pdf:pdf},
isbn = {0090-3973},
issn = {00903973},
journal = {Journal of Testing and Evaluation},
keywords = {associate professor,bibliometric,business and operations management,data mining,graduate school of,intellectual structure,rojo,social network analysis,tourism destination},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
month = {jan},
number = {1},
pages = {20120285},
title = {{Bibliometric and Social Network Analysis for Data Mining: The Intellectual Structure of Tourism Destination Literature}},
url = {http://www.astm.org/doiLink.cgi?JTE20120285},
volume = {42},
year = {2014}
}
@article{Bagheri2012a,
abstract = {Feature modeling an attractive technique for capturing commonality as well as variability within an application domain for generative programming and software product line engineering. Feature models symbolize an overarching representation of the possible application configuration space, and can hence be customized based on specific domain requirements and stakeholder goals. Most interactive or semiautomated feature model customization processes neglect the need to have a holistic approach towards the integration and satisfaction of the stakeholder's soft and hard constraints, and the application-domain integrity constraints. In this paper, we will show how the structure and constraints of a feature model can be modeled uniformly through Propositional Logic extended with concrete domains, called P(N). Furthermore, we formalize the representation of soft constraints in fuzzy P(N) and explain how semiautomated feature model customization is performed in this setting. The model configuration derivation process that we propose respects the soundness and completeness properties. {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
author = {Bagheri, Ebrahim and Noia, Tommaso Di and Gasevic, Dragan and Ragone, Azzurra},
doi = {10.1002/smr.534},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/ContentServer (1).pdf:pdf},
isbn = {9781450330565},
issn = {20477473},
journal = {Journal of Software: Evolution and Process},
keywords = {Feature models,Logic languages,Soft constraints,Software product lines,Variability,gris,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris,rojo},
month = {jun},
number = {4},
pages = {375--400},
pmid = {67195556},
title = {{Formalizing interactive staged feature model configuration}},
url = {http://doi.wiley.com/10.1002/smr.534},
volume = {24},
year = {2012}
}
@book{Alaimo2013,
address = {BA},
author = {Alaimo, Diego Mart{\'{i}}n},
edition = {1},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Alaimo - 2013 - Proyectos {\'{a}}giles con {\#}Scrum flexibilidad, aprendizaje, innovaci{\'{o}}n y colaboraci{\'{o}}n en contextos complejos.pdf:pdf},
isbn = {9789874515810},
mendeley-groups = {propuesta,informe},
pages = {123},
publisher = {Kleer},
title = {{Proyectos {\'{a}}giles con {\#}Scrum : flexibilidad, aprendizaje, innovaci{\'{o}}n y colaboraci{\'{o}}n en contextos complejos}},
year = {2013}
}
@article{Chen2014,
abstract = {The Three Gorges region of central western China is one of the most landslide-prone regions in the world. However, landslide detection based on field surveys and optical remote sensing and synthetic aperture radar (SAR) techniques remains difficult owing to the dense vegetation cover and mountain shadow. In the present study, an area of Zigui County in the Three Gorges region was selected to test the feasibility of detecting landslides by employing novel features extracted from a LiDAR-derived DTM. Additionally, two small sites-Site 1 and Site 2-were selected for training and were used to classify each other. In addition to the aspect, DTM, and slope images, the following feature sets were proposed to improve the accuracy of landslide detection: (1) the mean aspect, DTM, and slope textures based on four texture directions; (2) aspect, DTM, and slope textures based on aspect; and (3) the moving average and standard deviation (stdev) filter of aspect, DTM, and slope. By combining a feature selection method and the RF algorithm, the classification accuracy was evaluated and landslide boundaries were determined. The results can be summarized as follows. (1) The feature selection method demonstrated that the proposed features provided information useful for effective landslide identification. (2) Feature selection achieved an improvement of about 0.44{\%} in the overall classification accuracy, with the feature set reduced by 74{\%}, from 39 to 10; this can speed up the training of the RF model. (3) When fifty randomly selected 20{\%} of landslide pixels (PLS) and 20{\%} of non-landslide pixels (PNLS) (i.e., 20{\%} of PLS and PNLS) were utilized in addition to the selected feature subsets for training, the test sets (i.e., the remaining 80{\%} of PLS and PNLS) yielded an average overall classification accuracy of 78.24{\%}. The cross training and classification for Site 1 and Site 2 provided overall classification accuracies of 62.65{\%} and 64.50{\%}, respectively. This shows that the random sampling design (which suffered some of the effects of spatial auto-correlation) and the proposed method in this present study contribute jointly to the classification accuracy. (4) Using the Canny operator to delineate landslide boundaries based on the classification results of PLS and PNLS, we obtained results consistent with the referenced landslide inventory maps. Thus, the proposed procedure, which combines LiDAR data, a feature selection method, and the RF algorithm, can identify forested landslides effectively in the Three Gorges region. {\textcopyright} 2014 Elsevier Inc.},
author = {Chen, Weitao and Li, Xianju and Wang, Yanxin and Chen, Gang and Liu, Shengwei},
doi = {10.1016/j.rse.2014.07.004},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/1-s2.0-S0034425714002491-main.pdf:pdf},
isbn = {00344257},
issn = {00344257},
journal = {Remote Sensing of Environment},
keywords = {Feature selection,Landslide mapping,LiDAR,Random forest,The Three Gorges,Topographic analysis,purpura},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura},
month = {sep},
pages = {291--301},
publisher = {Elsevier Inc.},
title = {{Forested landslide detection using LiDAR data and the random forest algorithm: A case study of the Three Gorges, China}},
url = {http://dx.doi.org/10.1016/j.rse.2014.07.004 http://linkinghub.elsevier.com/retrieve/pii/S0034425714002491},
volume = {152},
year = {2014}
}
@article{Xue2013,
abstract = {Model reduction is an important systems task with a long history in traditional chemical engineering modeling. We discuss its interplay with modern data-mining tools (such as Local Feature Analysis and Diffusion Maps) through illustrative examples, and comment on important open issues regarding applications to large systems arising in molecular/atomistic simulations.},
author = {Xue, Yuzhen and Ludovice, Peter J. and Grover, Martha A. and Nedialkova, Lilia V. and Dsilva, Carmeline J. and Kevrekidis, Ioannis G.},
doi = {10.1016/j.compchemeng.2012.06.029},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Xue et al. - 2013 - State reduction in molecular simulations.pdf:pdf},
issn = {00981354},
journal = {Computers {\&} Chemical Engineering},
keywords = {Data mining,Diffusion Maps,Local Feature Analysis,Model reduction,Principal Component Analysis,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
month = {apr},
pages = {102--110},
title = {{State reduction in molecular simulations}},
url = {http://www.sciencedirect.com/science/article/pii/S009813541200213X http://linkinghub.elsevier.com/retrieve/pii/S009813541200213X},
volume = {51},
year = {2013}
}
@article{EdwardsM.2015,
author = {{Edwards M.}, Rashid A Rayson P},
doi = {10.1145/2811403},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/edwards2015.pdf:pdf},
issn = {03600300 (ISSN)},
journal = {ACM Computing Surveys},
keywords = {naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
number = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84945398245{\&}partnerID=40{\&}md5=328a99120df9ccad49817e34c6c79c2b},
title = {{A systematic survey of online data mining technology intended for law enforcement}},
volume = {48,1, 15,,},
year = {2015}
}
@article{Afzal2016,
abstract = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product.},
author = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
doi = {10.1016/j.csi.2016.03.003},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/10.1016@j.csi.2016.03.003.pdf:pdf},
issn = {09205489},
journal = {Computer Standards {\&} Interfaces},
keywords = {Artificial intelligence,Automated feature selection,Inconsistencies,Industrial SPL tools,Literature review,Predictive analytics,Software product line,purpura,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura,rojo},
month = {nov},
pages = {30--48},
publisher = {Elsevier B.V.},
title = {{Intelligent software product line configurations: A literature review}},
url = {http://dx.doi.org/10.1016/j.csi.2016.03.003 http://linkinghub.elsevier.com/retrieve/pii/S0920548916300198},
volume = {48},
year = {2016}
}
@inproceedings{Nieke2016,
address = {New York, New York, USA},
author = {Nieke, Michael and Seidl, Christoph and Schuster, Sven},
booktitle = {Proceedings of the Tenth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '16},
doi = {10.1145/2866614.2866625},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/nieke2016.pdf:pdf},
isbn = {9781450340199},
keywords = {Configuration,Evolution,Software Product Line (SPL),Temporal Feature Model (TFM),gris},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris},
pages = {73--80},
publisher = {ACM Press},
title = {{Guaranteeing Configuration Validity in Evolving Software Product Lines}},
url = {http://dl.acm.org/citation.cfm?id=2866614.2866625 http://dl.acm.org/citation.cfm?doid=2866614.2866625},
year = {2016}
}
@article{Koutanaei2015,
abstract = {Data mining techniques have numerous applications in credit scoring of customers in the banking field. One of the most popular data mining techniques is the classification method. Previous researches have demonstrated that using the feature selection (FS) algorithms and ensemble classifiers can improve the banks' performance in credit scoring problems. In this domain, the main issue is the simultaneous and the hybrid utilization of several FS and ensemble learning classification algorithms with respect to their parameters setting, in order to achieve a higher performance in the proposed model. As a result, the present paper has developed a hybrid data mining model of feature selection and ensemble learning classification algorithms on the basis of three stages. The first stage, as expected, deals with the data gathering and pre-processing. In the second stage, four FS algorithms are employed, including principal component analysis (PCA), genetic algorithm (GA), information gain ratio, and relief attribute evaluation function. In here, parameters setting of FS methods is based on the classification accuracy resulted from the implementation of the support vector machine (SVM) classification algorithm. After choosing the appropriate model for each selected feature, they are applied to the base and ensemble classification algorithms. In this stage, the best FS algorithm with its parameters setting is indicated for the modeling stage of the proposed model. In the third stage, the classification algorithms are employed for the dataset prepared from each FS algorithm. The results exhibited that in the second stage, PCA algorithm is the best FS algorithm. In the third stage, the classification results showed that the artificial neural network (ANN) adaptive boosting (AdaBoost) method has higher classification accuracy. Ultimately, the paper verified and proposed the hybrid model as an operative and strong model for performing credit scoring.},
author = {Koutanaei, Fatemeh Nemati and Sajedi, Hedieh and Khanbabaei, Mohammad},
doi = {10.1016/j.jretconser.2015.07.003},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Koutanaei, Sajedi, Khanbabaei - 2015 - A hybrid data mining model of feature selection algorithms and ensemble learning classifiers for.pdf:pdf},
issn = {09696989},
journal = {Journal of Retailing and Consumer Services},
keywords = {Classification,Credit scoring,Data mining,Ensemble learning,Feature selection,naranja},
mendeley-groups = {estado del arte,con doi,propuesta,informe},
mendeley-tags = {naranja},
month = {nov},
pages = {11--23},
title = {{A hybrid data mining model of feature selection algorithms and ensemble learning classifiers for credit scoring}},
url = {http://www.sciencedirect.com/science/article/pii/S0969698915300060 http://linkinghub.elsevier.com/retrieve/pii/S0969698915300060},
volume = {27},
year = {2015}
}
@inproceedings{Somprasertsri2010,
abstract = {In web pages, the reviews are written in natural language and are unstructured-free-texts scheme. Online product reviews is considered as a significant informative resource which is useful for both potential customers and product manufacturers. The task of manually scanning through large amounts of review one by one is computational burden and is not practically implemented with respect to businesses and customer perspectives. Therefore it is more efficient to automatically process the various reviews and provide the necessary information in a suitable form. The task of product feature and opinion is to find product features that customers refer to their topic reviews. It would be useful to characterize the opinions about product. In this paper, we propose an approach to extract product features and to identify the opinions associated with these features from reviews through syntactic information based on dependency analysis. {\textcopyright}2010 IEEE.},
author = {Sompras, Gamgarn and Lalitrojwong, Pattarachai},
booktitle = {2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery},
doi = {10.1109/FSKD.2010.5569865},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/05569865.pdf:pdf},
isbn = {978-1-4244-5931-5},
keywords = {Customer review,Dependency analysis,Opinion extraction,Opinion mining,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
month = {aug},
number = {Fskd},
pages = {2358--2362},
publisher = {IEEE},
title = {{Extracting product features and opinions from product reviews using dependency analysis}},
url = {http://ieeexplore.ieee.org/document/5569865/},
volume = {5},
year = {2010}
}
@article{Eleuterio2015,
abstract = {Software Product Lines (SPLs) are emerging techniques where several artifacts are reused (domain), and some are customized (variation points). An SPL can bind variation points statically (compilation time) or dynamically (runtime). Dynamic Software Product Lines (DSPLs) us e dynamic bind ing to adapt to the environment or requirements changes. DSPLs are commonly used to build dependable systems, defined as systems with the ability to avoid service failures more frequent or severe than is acceptable. Main dependability attribu tes are availability, confidentiality, integrity, reliability, maintainability, and safety. To better understand this context, a Systematic Mapping Study (SMS) was applied searching proposals that include dependability attributes in DSPLs. Our results sugg est that few studies handle dependability in DSPL context. We selected only nine primary studies in this context. Also, four of them provide explicit support for SOA and were analyzed as a secondary result},
author = {Eleut{\'{e}}rio, Jane D A S and Gaia, Felipe N and Rodrigues, Gena{\'{i}}na N and Rubira, Cec{\'{i}}lia M F},
doi = {10.13140/RG.2.1.2717.4880},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/15-04.pdf:pdf},
keywords = {purpura},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura},
number = {April},
title = {{Dependable Dynamic Software Product Line--a Systematic Mapping Study}},
year = {2015}
}
@inproceedings{Yang2015a,
author = {{Guanzhong Yang} and {Yaru Zhang}},
booktitle = {2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)},
doi = {10.1109/FSKD.2015.7382185},
file = {:Users/josepplloo/Downloads/07382185.pdf:pdf},
isbn = {978-1-4673-7682-2},
keywords = {-,concept,domain,embedded,feature modeling,gris,naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris,naranja},
month = {aug},
pages = {1607--1612},
publisher = {IEEE},
title = {{A feature-oriented modeling approach for embedded product line engineering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7382185},
year = {2015}
}
@article{Pablo2012,
author = {Pablo, S and Garc, Diego and Zorrilla, Marta and Matem, Dpto},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/06403212.pdf:pdf},
isbn = {978-1-4673-4743-3},
journal = {Proceedings of the 2012 International Symposium on Computers in Education (SIIE)},
keywords = {gris,purpura},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris,purpura},
pages = {1 -- 6},
title = {{Software Product Line Engineering for e-Learning Applications : A Case Study}},
year = {2012}
}
@inproceedings{Bezerra2016,
address = {New York, New York, USA},
author = {Bezerra, Carla I M and Monteiro, Jos{\'{e}} Maria and Andrade, Rossana M C and Rocha, Lincoln S},
booktitle = {Proceedings of the Tenth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '16},
doi = {10.1145/2866614.2866617},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/bezerra2016.pdf:pdf},
isbn = {9781450340199},
keywords = {Evolution,Feature Model,Maintainability,naranja,purpura},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,purpura},
pages = {17--24},
publisher = {ACM Press},
title = {{Analyzing the Feature Models Maintainability over their Evolution Process}},
url = {http://doi.acm.org/10.1145/2866614.2866617 http://dl.acm.org/citation.cfm?doid=2866614.2866617},
year = {2016}
}
@misc{Vaishnavi2013,
author = {Vaishnavi, Vijay and Kuechler, Bill},
booktitle = {2004},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Vaishnavi, Kuechler - 2013 - Design Science Research in Information Systems.pdf:pdf},
mendeley-groups = {propuesta,informe},
pages = {54},
title = {{Design Science Research in Information Systems}},
year = {2013}
}
@article{Fenske2015,
abstract = {Highly-configurable software systems (also called software product lines) gain momentum in both, academia and industry. For instance, the Linux kernel comes with over 12 000 configuration options and thus, can be customized to run on nearly every kind of system. To a large degree, this configurability is achieved through variable code structures, for instance, using conditional compilation. Such source code variability adds a new dimension of complexity, thus giving rise to new possibilities for design flaws. Code smells are an established concept to describe design flaws or decay in source code. However, existing smells have no notion of variability and thus do not support flaws regarding variable code structures. In this paper, we propose an initial catalog of four variability-aware code smells. We discuss the appearance and negative effects of these smells and present code examples from real-world systems. To evaluate our catalog, we have conducted a survey amongst 15 researchers from the field of software product lines. The results confirm that our proposed smells (a) have been observed in existing product lines and (b) are considered to be problematic for common software development activities, such as program comprehension, maintenance, and evolution.},
author = {Fenske, Wolfram and Schulze, Sandro},
doi = {10.1145/2701319.2701321},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/fenske2015.pdf:pdf},
isbn = {978-1-4503-3273-6},
journal = {Proceedings of the Ninth International Workshop on Variability Modelling of Software-intensive Systems},
keywords = {Code Smells,Design Defects,Software Product Lines,Variability,naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
pages = {3:3----3:10},
title = {{Code Smells Revisited: A Variability Perspective}},
url = {http://doi.acm.org/10.1145/2701319.2701321},
year = {2015}
}
@article{Song2002,
author = {Song, Shuang and Dong, Andy and Agogino, Alice},
doi = {10.1115/1.1528921},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/asme/199{\_}1.pdf:pdf},
issn = {15309827},
journal = {Journal of Computing and Information Science in Engineering},
keywords = {purpura},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura},
number = {3},
pages = {199},
title = {{Modeling Information Needs in Engineering Databases Using Tacit Knowledge}},
url = {http://computingengineering.asmedigitalcollection.asme.org/article.aspx?articleid=1399467},
volume = {2},
year = {2002}
}
@inproceedings{Seidl2013,
abstract = {Software product lines (SPLs) and software ecosystems (SECOs) are approaches to capturing families of closely related software systems in terms of common and variable functionality. SPLs and especially SECOs are subject to evolution to adapt to new or changed requirements resulting in different versions of the software family and its variable assets. These versions may have to be maintained and used for products even after they were superseded by newer versions. Variability models describing valid combinations of variable assets, such as feature models, capture variability in space (configuration), but not variability in time (evolution) making it impossible to respect versions of variable assets in product definitions on a conceptual level. In this paper, we propose Hyper Feature Models (HFMs) explicitly providing feature versions as configurable units for product definition. Furthermore, we provide a version-aware constraint language to specify dependencies between features and ranges of feature versions as well as a procedure to automatically select valid combinations of versions for a pre-configuration of features. We demonstrate our approach in a case study. {\textcopyright} 2014 ACM.},
address = {New York, New York, USA},
author = {Seidl, Christoph and Schaefer, Ina and A{\ss}mann, Uwe},
booktitle = {Proceedings of the Eighth International Workshop on Variability Modelling of Software-Intensive Systems - VaMoS '14},
doi = {10.1145/2556624.2556625},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/seidl2013.pdf:pdf},
isbn = {9781450325561},
keywords = {evolution,gris,hfm,hyper feature model,seco,software ecosystem,software product line,spl,variability in time},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris},
pages = {1--8},
publisher = {ACM Press},
title = {{Capturing variability in space and time with hyper feature models}},
url = {http://dl.acm.org/citation.cfm?doid=2556624.2556625},
year = {2013}
}
@article{Qian2015,
author = {Qian, Wenbin and Shu, Wenhao},
doi = {10.1016/j.neucom.2015.05.105},
file = {:Users/josepplloo/Documents/primer inmersion/Mutual information criterion for feature selection from incomplete data.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {gris,naranja},
mendeley-groups = {estado del arte,con doi,propuesta,informe},
mendeley-tags = {gris,naranja},
month = {nov},
pages = {210--220},
title = {{Mutual information criterion for feature selection from incomplete data}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215008036},
volume = {168},
year = {2015}
}
@book{Simpson,
address = {New York, NY},
author = {Simpson, Timothy W and Jiao, Jianxin Roger},
doi = {10.1007/978-1-4614-7937-6},
editor = {Simpson, Timothy W. and Jiao, Jianxin and Siddique, Zahed and H{\"{o}}ltt{\"{a}}-Otto, Katja},
file = {:Users/josepplloo/Downloads/Advances in Product Family and Product Platform Design.pdf:pdf},
isbn = {978-1-4614-7936-9},
keywords = {naranja},
mendeley-groups = {maerial base para el libro,con doi,informe},
mendeley-tags = {naranja},
publisher = {Springer New York},
title = {{Advances in Product Family and Product Platform Design}},
url = {http://link.springer.com/10.1007/978-1-4614-7937-6},
year = {2014}
}
@inproceedings{Janota2013,
address = {New York, New York, USA},
author = {Janota, Mikol{\'{a}}{\v{s}} and Botterweck, Goetz and Marques-Silva, Joao},
booktitle = {Proceedings of the Eighth International Workshop on Variability Modelling of Software-Intensive Systems - VaMoS '14},
doi = {10.1145/2556624.2556644},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/janota2013.pdf:pdf},
isbn = {9781450325561},
keywords = {interactive configuration,minimal correction sets,naranja,purpura,sat},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,purpura},
pages = {1--8},
publisher = {ACM Press},
title = {{On lazy and eager interactive reconfiguration}},
url = {http://dl.acm.org/citation.cfm?doid=2556624.2556644},
year = {2013}
}
@inproceedings{Simmonds2016,
address = {New York, New York, USA},
author = {Blum, Fabian Rojas and Simmonds, Jocelyn and Bastarrica, Mar{\'{i}}a Cecilia},
booktitle = {Proceedings of the 2015 International Conference on Software and System Process - ICSSP 2015},
doi = {10.1145/2785592.2785605},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/blum2015.pdf:pdf},
isbn = {9781450333467},
keywords = {naranja,noise,process discovery,software process lines,variability},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
number = {August 2015},
pages = {127--136},
publisher = {ACM Press},
title = {{Software process line discovery}},
url = {http://dl.acm.org/citation.cfm?doid=2785592.2785605},
year = {2015}
}
@book{Hastie2015,
author = {Hastie, Trevor and Tibshirani, Robert and Wainwright, Martin},
file = {:Users/josepplloo/Documents/primer inmersion/libros de aprendizaje estadistico/Hastie, Trevor-Statistical learning with sparsity {\_} the lasso and generalizations.-Chapman {\&} Hall Crc (2015).pdf:pdf},
isbn = {9781498712170},
mendeley-groups = {estado del arte,con doi,informe},
title = {{Statistical Learning with Sparsity}},
year = {2015}
}
@inproceedings{Valov2015,
address = {New York, New York, USA},
author = {Valov, Pavel and Guo, Jianmei and Czarnecki, Krzysztof},
booktitle = {Proceedings of the 19th International Conference on Software Product Line - SPLC '15},
doi = {10.1145/2791060.2791069},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/valov2015.pdf:pdf},
isbn = {9781450336130},
keywords = {purpura},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura},
pages = {186--190},
publisher = {ACM Press},
title = {{Empirical comparison of regression methods for variability-aware performance prediction}},
url = {http://dl.acm.org/citation.cfm?id=2791060.2791069 http://dl.acm.org/citation.cfm?doid=2791060.2791069},
year = {2015}
}
@article{Hamraz2012,
abstract = {Engineering change (EC) is a source of uncertainty. While the number of changes to a design can be optimized, their existence cannot be eliminated. Each change is accompanied by intended and unintended impacts both of which might propagate and cause further knock-on changes. Such change propagation causes uncertainty in design time, cost, and quality and thus needs to be predicted and controlled. Current engineering change propagation models map the product connectivity into a single-domain network and model change propagation as spread within this network. Those models miss out most dependencies from other domains and suffer from {\^{a}}€{\oe}hidden dependencies{\^{a}}€ . This paper proposes the function-behavior-structure (FBS) linkage model, a multidomain model which combines concepts of both the function-behavior-structure model from Gero and colleagues with the change prediction method (CPM) from Clarkson and colleagues. The FBS linkage model is represented in a network and a corresponding multidomain matrix of structural, behavioral, and functional elements and their links. Change propagation is described as spread in that network using principles of graph theory. The model is applied to a diesel engine. The results show that the FBS linkage model is promising and improves current methods in several ways: The model (1) accounts explicitly for all possible dependencies between product elements, (2) allows capturing and modeling of all relevant change requests, (3) improves the understanding of why and how changes propagate, (4) is scalable to different levels of decomposition, and (5) is flexible to present the results on different levels of abstraction. All these features of the FBS linkage model can help control and counteract change propagation and reduce uncertainty and risk in design.},
author = {Hamraz, Bahram and Caldwell, Nicholas H. M. and {John Clarkson}, P.},
doi = {10.1115/1.4007397},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/asme/100905{\_}1.pdf:pdf},
isbn = {1050-0472},
issn = {10500472},
journal = {Journal of Mechanical Design},
keywords = {change propagation,design structure matrix,engineering change management,functional reasoning,gris,mechanical design,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris,rojo},
number = {10},
pages = {100905},
title = {{A Multidomain Engineering Change Propagation Model to Support Uncertainty Reduction and Risk Management in Design}},
url = {http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?articleid=1484826},
volume = {134},
year = {2012}
}
@article{Felfernig2015,
abstract = {Automated testing and debugging of knowledge bases (such as configuration knowledge bases and feature models) is an important contribution to manage knowledge evolution efficiently. However, existing approaches rely on the assumption of consistent test suites which are always kept up-to-date within the scope of different knowledge base maintenance cycles. In this paper we introduce diagnosis techniques that actively guide stakeholders (knowledge engineers and domain experts) in the process of testing and debugging knowledge bases. These techniques take into account faulty test cases and constraints and recommend diagnoses which are the source of a given inconsistency.},
author = {Felfernig, Alexander and Reiterer, Stefan and Stettinger, Martin and Tiihonen, Juha},
doi = {10.1145/2701319.2701320},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/felfernig2015.pdf:pdf},
isbn = {9781450332736},
journal = {Proceedings of the Ninth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '15},
keywords = {Automated Debugging,Configuration,Feature Models,purpura,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura,rojo},
pages = {51--58},
title = {{Intelligent Techniques for Configuration Knowledge Evolution}},
url = {http://dl.acm.org/citation.cfm?id=2701319.2701320},
year = {2015}
}
@article{Sepulveda2016,
abstract = {Context: Software product lines (SPLs) have reached a considerable level of adoption in the software industry, having demonstrated their cost-effectiveness for developing higher quality products with lower costs. For this reason, in the last years the requirements engineering community has devoted much effort to the development of a myriad of requirements modelling languages for SPLs. Objective: In this paper, we review and synthesize the current state of research of requirements modelling languages used in SPLs with respect to their degree of empirical validation, origin and context of use, level of expressiveness, maturity, and industry adoption. Method: We have conducted a systematic literature review with six research questions that cover the main objective. It includes 54 studies, published from 2000 to 2013. Results: The mean level of maturity of the modelling languages is 2.59 over 5, with 46{\%} of them falling within level 2 or below -no implemented abstract syntax reported-. They show a level of expressiveness of 0.7 over 1.0. Some constructs (feature, mandatory, optional, alternative, exclude and require) are present in all the languages, while others (cardinality, attribute, constraint and label) are less common. Only 6{\%} of the languages have been empirically validated, 41{\%} report some kind of industry adoption and 71{\%} of the languages are independent from any development process. Last but not least, 57{\%} of the languages have been proposed by the academia, while 43{\%} have been the result of a joint effort between academia and industry. Conclusions: Research on requirements modeling languages for SPLs has generated a myriad of languages that differ in the set of constructs provided to express SPL requirements. Their general lack of empirical validation and adoption in industry, together with their differences in maturity, draws the picture of a discipline that still needs to evolve.},
author = {Sep{\'{u}}lveda, Samuel and Cravero, Ania and Cachero, Cristina},
doi = {10.1016/j.infsof.2015.08.007},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/10.1016@j.infsof.2015.08.007.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Modeling languages,Requirements engineering,Software product lines,Systematic literature review,naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
month = {jan},
pages = {16--36},
title = {{Requirements modeling languages for software product lines: A systematic literature review}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950584915001494},
volume = {69},
year = {2016}
}
@inproceedings{Martinez2014,
abstract = {Software Product Line Engineering is a mature approach enabling the derivation of product variants by assembling reusable assets. In this context, domain experts widely use Feature Models as the most accepted formalism for capturing commonality and variability in terms of features. Feature Models also describe the constraints in feature combinations. In industrial settings, domain experts often deal with Software Product Lines with high numbers of features and constraints. Furthermore, the set of features are often regrouped in different subsets that are overseen by different stakeholders in the process. Consequently, the management of the complexity of large Feature Models becomes challenging. In this paper we propose a dedicated interactive visualisation paradigm to help domain experts and stakeholders to manage the challenges in maintaining the constraints among features. We build Feature Relations Graphs (Frogs) by mining existing product configurations. For each feature, we are able to display a Frog which shows the impact, in terms of constraints, of the considered feature on all the other features. The objective is to help domain experts to 1) obtain a better understanding of feature constraints, 2) potentially refine the existing feature model by uncovering and formalizing missing constraints and 3) serve as a recommendation system, during the configuration of a new product, based on the tendencies found in existing configurations. The paper illustrates the visualisation paradigm with the industrial case study of Renault's Electric Parking System Software Product Line.},
author = {Martinez, Jabier and Ziadi, Tewfik and Mazo, Raul and Bissyande, Tegawende F. and Klein, Jacques and Traon, Yves Le},
booktitle = {2014 Second IEEE Working Conference on Software Visualization},
doi = {10.1109/VISSOFT.2014.18},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/martinez2014.pdf:pdf},
isbn = {978-1-4799-6150-4},
keywords = {rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
month = {sep},
pages = {50--59},
publisher = {IEEE},
title = {{Feature Relations Graphs: A Visualisation Paradigm for Feature Constraints in Software Product Lines}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6980213},
year = {2014}
}
@inproceedings{Davril2015b,
author = {Davril, Jean-Marc and Cordy, Maxime and Heymans, Patrick and Acher, Mathieu},
booktitle = {2015 IEEE Second International Workshop on Artificial Intelligence for Requirements Engineering (AIRE)},
doi = {10.1109/AIRE.2015.7337624},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/07337624.pdf:pdf},
isbn = {978-1-5090-0125-5},
keywords = {Cameras,Documentation,Frequency modulation,Portable computers,Pragmatics,Product design,Quality assessment,purpura},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura},
month = {aug},
pages = {1--8},
publisher = {IEEE},
title = {{Using fuzzy modeling for consistent definitions of product qualities in requirements}},
url = {http://ieeexplore.ieee.org/document/7337624/},
year = {2015}
}
@article{GIRALDO2014,
abstract = {Feature Models (FMs) are a notation to represent differences and commonalities between products derived from a product line. However, product line modelers could unintentionally incorporate dead features in FMs. A dead feature is a type of defect, which implies that one or more features are not present in any product of the product line. Some authors have used ontologies in product lines, but they have not exploited ontology reasoning to identify and explain causes for defects in FMs in natural language. In this paper, we propose an ontology that represents FMs in OWL (Web Ontology Language). Then, we use SQWRL (Semantic Query-enhanced Web Rule Language) to identify dead features in a FM and identify and explain certain causes of this defect in natural language. Our preliminary empirical evaluation confirms the benefits of our approach.},
author = {GIRALDO, GLORIA LUCIA and RINC{\'{O}}N-PEREZ, LUISA and MAZO, RAUL},
doi = {10.15446/dyna.v81n183.36348},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/SciELO/v81n183a08.pdf:pdf},
issn = {2346-2183},
journal = {DYNA},
keywords = {Product lines,SQWRL.,dead features,feature models,ontologies,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
month = {jan},
number = {183},
pages = {68},
title = {{IDENTIFYING DEAD FEATURES AND THEIR CAUSES IN PRODUCT LINE MODELS: AN ONTOLOGICAL APPROACH}},
url = {http://dyna.medellin.unal.edu.co/en/verResumenEN.php?id{\_}articulo=v81n183a08 http://www.revistas.unal.edu.co/index.php/dyna/article/view/36348},
volume = {81},
year = {2014}
}
@inproceedings{Yamany2015,
author = {Yamany, Ahmed Eid El and Elgamel, Mohamed Shaheen},
booktitle = {The 7th International Conference on Information Technology},
doi = {10.15849/icit.2015.0114},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/0114.pdf:pdf},
isbn = {9789957858339},
keywords = {feature models,gris,machine learning,multi-objective optimization,non-dominant solutions,optimal feature selection,pareto front,search-based software engineering,software product lines,uil,user-in-the-loop},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris},
month = {may},
pages = {657--666},
publisher = {Al-Zaytoonah University of Jordan},
title = {{Smart OptiSelect Preference Based Innovative Framework for User-in-the-Loop Feature Selection in Software Product Lines}},
url = {http://icit.zuj.edu.jo/icit15/DOI/Software{\_}Engineering/0114.pdf},
volume = {2015},
year = {2015}
}
@inproceedings{Niu2008,
abstract = {We propose an on-demand clustering framework for analyzing the functional requirements in a product line. Our approach is novel in that the objects to be clustered capture the domain's action themes at a primitive level, and the essential attributes are uncovered via semantic analysis. We provide automatic support to complement domain analysis by quickly identifying important entities and functionalities. A second contribution is our recognition of stakeholders' different goals in cluster analysis, e.g., feature identification for users versus system decomposition for designers. We thus advance the literature by examining requirements clusters that overlap and those causing a minimal information loss, and by facilitating the discovery of product line variabilities. A proof-of-concept example is presented to show the applicability and usefulness of our approach.},
author = {Niu, Nan and Easterbrook, Steve},
booktitle = {2008 12th International Software Product Line Conference},
doi = {10.1109/SPLC.2008.11},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/04626843.pdf:pdf},
isbn = {978-0-7695-3303-2},
keywords = {DP industry,on demand cluster analysis,pattern clustering,product line functional requirements,purpura,semantic analysis,software engineering,software product line engineering,statistical analysis,systems analysis},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura},
month = {sep},
pages = {87--96},
publisher = {IEEE},
title = {{On-Demand Cluster Analysis for Product Line Functional Requirements}},
url = {http://ieeexplore.ieee.org/document/4626843/},
year = {2008}
}
@inproceedings{Capilla2015,
address = {New York, New York, USA},
author = {Capilla, Rafael and Hinchey, Mike and D{\'{i}}az, Francisco J.},
booktitle = {Proceedings of the Ninth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '15},
doi = {10.1145/2701319.2701322},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/capilla2015.pdf:pdf},
isbn = {9781450332736},
keywords = {Feature modeling,adaptation,context features,context-aware systems,rojo,runtime},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
pages = {43--50},
publisher = {ACM Press},
title = {{Collaborative Context Features for Critical Systems}},
url = {http://dl.acm.org/citation.cfm?id=2701319.2701322 http://dl.acm.org/citation.cfm?doid=2701319.2701322},
year = {2015}
}
@article{Wang2014,
abstract = {In the context of product lines, test case selection aims at obtaining a set of relevant test cases for a product from the entire set of test cases available for a product line. While working on a research-based innovation project on automated testing of product lines of Video Conferencing Systems (VCSs) developed by Cisco, we felt the need to devise a cost-effective way of selecting relevant test cases for a product. To fulfill such need, we propose a systematic and automated test selection methodology using: 1) Feature Model for Testing (FM{\_}T) to capture commonalities and variabilities of a product line; 2) Component Family Model for Testing (CFM{\_}T) to model the structure of test case repository; 3) A tool to automatically build restrictions from CFM{\_}T to FM{\_}T and traces from CFM{\_}T to the actual test cases. Using our methodology, a test engineer is only required to select relevant features through FM{\_}T at a higher level of abstraction for a product and the corresponding test cases will be obtained automatically. We evaluate our methodology by applying it to a VCS product line called Saturn with seven commercial products and the results show that our methodology can significantly reduce cost measured as test selection time and at the same time achieves higher effectiveness (feature coverage, feature pairwise coverage and fault detection) as compared with the current manual process. Moreover, we conduct a questionnaire-based study to solicit the views of test engineers who are involved in developing FM{\_}T and CFM{\_}T. The results show that test engineers are positive about adapting our methodology in their current practice. Finally, we present a set of lessons learnt while applying product line engineering at Cisco for test case selection.},
author = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud and Liaaen, Marius},
doi = {10.1007/s10664-014-9345-5},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/springer/wang2014.pdf:pdf},
isbn = {13823256 (ISSN)},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Component family model,Feature model,Product line,Test case selection,gris,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris,rojo},
month = {aug},
number = {4},
pages = {1586--1622},
title = {{A systematic test case selection methodology for product lines: results and insights from an industrial case study}},
url = {http://link.springer.com/10.1007/s10664-014-9345-5},
volume = {21},
year = {2016}
}
@article{Liang2015a,
abstract = {Financial distress prediction is always important for financial institutions in order for them to assess the financial health of enterprises and individuals. Bankruptcy prediction and credit scoring are two important issues in financial distress prediction where various statistical and machine learning techniques have been employed to develop financial prediction models. Since there are no generally agreed upon financial ratios as input features for model development, many studies consider feature selection as a pre-processing step in data mining before constructing the models. However, most works only focused on applying specific feature selection methods over either bankruptcy prediction or credit scoring problem domains. In this work, a comprehensive study is conducted to examine the effect of performing filter and wrapper based feature selection methods on financial distress prediction. In addition, the effect of feature selection on the prediction models obtained using various classification techniques is also investigated. In the experiments, two bankruptcy and two credit datasets are used. In addition, three filter and two wrapper based feature selection methods combined with six different prediction models are studied. Our experimental results show that there is no the best combination of the feature selection method and the classification technique over the four datasets. Moreover, depending on the chosen techniques, performing feature selection does not always improve the prediction performance. However, on average performing the genetic algorithm and logistic regression for feature selection can provide prediction improvements over the credit and bankruptcy datasets respectively.},
author = {Liang, Deron and Tsai, Chih-Fong and Wu, Hsin-Ting},
doi = {10.1016/j.knosys.2014.10.010},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Liang, Tsai, Wu - 2015 - The effect of feature selection on financial distress prediction.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Bankruptcy prediction,Credit scoring,Data mining,Feature selection,Financial distress prediction,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
month = {jan},
pages = {289--297},
title = {{The effect of feature selection on financial distress prediction}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705114003773},
volume = {73},
year = {2015}
}
@inproceedings{Weyns2014,
address = {New York, New York, USA},
author = {Weyns, Danny},
booktitle = {Proceedings of the 18th International Software Product Line Conference on Companion Volume for Workshops, Demonstrations and Tools - SPLC '14},
doi = {10.1145/2647908.2655959},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/weyns2014.pdf:pdf},
isbn = {9781450327398},
keywords = {naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
pages = {12--12},
publisher = {ACM Press},
title = {{Variability}},
url = {http://dl.acm.org/citation.cfm?doid=2647908.2655959},
year = {2014}
}
@inproceedings{M.Harman2014,
abstract = {This paper1 presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that ap- ply computational search techniques to problems in software product line engineering. Having surveyed the recent explo- sion in SBSE for SPL research activity, we highlight some di- rections for future work. We focus on suggestions for the de- velopment of recent advances in genetic improvement, show- ing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new prod- ucts with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
address = {New York, New York, USA},
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
booktitle = {Proceedings of the 18th International Software Product Line Conference on - SPLC '14},
doi = {10.1145/2648511.2648513},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/harman2014.pdf:pdf},
isbn = {9781450327404},
issn = {9781450327404 (ISBN)},
keywords = {genetic programming,program synthesis,rojo,sbse,spl},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
pages = {5--18},
publisher = {ACM Press},
title = {{Search based software engineering for software product line engineering}},
url = {http://doi.acm.org/10.1145/2648511.2648513 http://dl.acm.org/citation.cfm?doid=2648511.2648513},
volume = {1},
year = {2014}
}
@article{Comunicaciones2014,
author = {las Comunicaciones, Ministerio de Tecnolog{\'{i}}as de la Informaci{\'{o}}n y},
keywords = {MINTIC},
mendeley-groups = {estado del arte,propuesta,informe},
pages = {177},
title = {{INFORME RENDICI{\'{O}}N DE CUENTAS Ministerio de Tecnolog{\'{i}}as de la Informaci\'{o}}n y las Comunicaciones – Fondo de Tecnolog{\'{i}}as de la Informaci{\'{o}}n y las Comunicaciones}},
url = {http://www.mintic.gov.co/portal/604/articles-6423{\_}recurso{\_}5.pdf},
year = {2014}
}
@book{BoughzalaI.;OhO.;Reiter-Palmon2013,
address = {Berlin, Heidelberg},
author = {{Boughzala, I.; Oh, O.; Reiter-Palmon}, R.},
booktitle = {Collaboration and Technology},
doi = {10.1007/978-3-642-41347-6},
editor = {Antunes, Pedro and Gerosa, Marco Aur{\'{e}}lio and Sylvester, Allan and Vassileva, Julita and de Vreede, Gert-Jan},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/springer/(Lecture Notes in Computer Science 8224) Liana Razmerita (auth.), Pedro Antunes, Marco Aur{\'{e}}lio Gerosa, Allan Sylvester, Julita Vassileva, Gert-Jan de Vreede (eds.)-Collaboration and Technology{\_} 19th I.pdf:pdf},
isbn = {978-3-642-41346-9},
keywords = {azul},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {azul},
number = {November},
pages = {94--109},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Collaboration and Technology}},
url = {http://link.springer.com/10.1007/978-3-642-41347-6},
volume = {8224},
year = {2013}
}
@article{Nanculef2014,
abstract = {We present a method for the classification of multi-labeled text documents explicitly designed for data stream applications that require to process a virtually infinite sequence of data using constant memory and constant processing time. Our method is composed of an online procedure used to efficiently map text into a low-dimensional feature space and a partition of this space into a set of regions for which the system extracts and keeps statistics used to predict multi-label text annotations. Documents are fed into the system as a sequence of words, mapped to a region of the partition, and annotated using the statistics computed from the labeled instances colliding in the same region. This approach is referred to as clashing. We illustrate the method in real-world text data, comparing the results with those obtained using other text classifiers. In addition, we provide an analysis about the effect of the representation space dimensionality on the predictive performance of the system. Our results show that the online embedding indeed approximates the geometry of the full corpus-wise TF and TF-IDF space. The model obtains competitive F measures with respect to the most accurate methods, using significantly fewer computational resources. In addition, the method achieves a higher macro-averaged F measure than methods with similar running time. Furthermore, the system is able to learn faster than the other methods from partially labeled streams.},
author = {{\~{N}}anculef, Ricardo and Flaounas, Ilias and Cristianini, Nello},
doi = {10.1016/j.eswa.2014.02.017},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/{\~{N}}anculef, Flaounas, Cristianini - 2014 - Efficient classification of multi-labeled text streams by clashing.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Data streams,Feature hashing,Massive data mining,Multi-label classification,Text classification,azul},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {azul},
month = {sep},
number = {11},
pages = {5431--5450},
title = {{Efficient classification of multi-labeled text streams by clashing}},
url = {http://www.sciencedirect.com/science/article/pii/S0957417414000803},
volume = {41},
year = {2014}
}
@book{VanDerLinden2007,
abstract = {Software product lines represent perhaps the most exciting paradigm shift in software development since the advent of high-level programming languages. Nowhere else in software engineering have we seen such breathtaking improvements in cost, quality, time to market, and developer productivity, often registering in the order-of-magnitude range. While the underlying concepts are straightforward enough - building a family of related products or systems by planned and careful reuse of a base of generalized software development assets - the devil can be in the details, as successful product line practice can involve organizational change, business process change, and technology change. The authors ideally combine academic research results with industrial real-world experiences, thus presenting a broad view on product line engineering so that both managers and technical specialists will benefit from reading it. After presenting a common framework for the description of the industrial case studies, they capture the wealth of knowledge that eight companies have gathered during the introduction of the software product line engineering approach in their daily practice. After reading this book, you will understand all the relevant aspects, regarding business, architecture, process, and organizational issues, of applying software product line engineering. If you consider using a product line approach in your organization, or if you want to improve your current practices you will find a rich set of useful information at your fingertips - from practitioners to practitioners.},
address = {Berlin, Heidelberg},
author = {van der Linden, Frank and Schmid, Klaus and Rommes, Eelco},
booktitle = {Software Product Lines in Action: The Best Industrial Practice in Product Line Engineering},
doi = {10.1007/978-3-540-71437-8},
file = {:Users/josepplloo/Documents/Material de base para todos los cap{\`{i}}tulos/Algunos libros y documentos externos de apoyo/Software product lines in action - a practical guide.pdf:pdf},
isbn = {978-3-540-71436-1},
keywords = {gris},
mendeley-groups = {maerial base para el libro,con doi,informe},
mendeley-tags = {gris},
pages = {1--333},
publisher = {Springer Berlin Heidelberg},
title = {{Software Product Lines in Action}},
url = {http://link.springer.com/10.1007/978-3-540-71437-8},
year = {2007}
}
@article{Sasikala2014,
abstract = {Selection of optimal features is an important area of research in medical data mining systems. In this paper we introduce an efficient four-stage procedure – feature extraction, feature subset selection, feature ranking and classification, called as Multi-Filtration Feature Selection (MFFS), for an investigation on the improvement of detection accuracy and optimal feature subset selection. The proposed method adjusts a parameter named “variance coverage” and builds the model with the value at which maximum classification accuracy is obtained. This facilitates the selection of a compact set of superior features, remarkably at a very low cost. An extensive experimental comparison of the proposed method and other methods using four different classifiers (Na{\"{i}}ve Bayes (NB), Support Vector Machine (SVM), multi layer perceptron (MLP) and J48 decision tree) and 22 different medical data sets confirm that the proposed MFFS strategy yields promising results on feature selection and classification accuracy for medical data mining field of research.},
author = {Sasikala, S. and {Appavu alias Balamurugan}, S. and Geetha, S.},
doi = {10.1016/j.aci.2014.03.002},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Sasikala, Appavu alias Balamurugan, Geetha - 2014 - Multi Filtration Feature Selection (MFFS) to improve discriminatory ability in clini.pdf:pdf},
issn = {22108327},
journal = {Applied Computing and Informatics},
keywords = {Biomedical classification,Medical data mining,Multi Filtration Feature Selection,Principal Component Analysis,Variance coverage factor,naranja,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,rojo},
month = {jul},
number = {2},
pages = {117--127},
title = {{Multi Filtration Feature Selection (MFFS) to improve discriminatory ability in clinical data set}},
url = {http://www.sciencedirect.com/science/article/pii/S2210832714000088 http://linkinghub.elsevier.com/retrieve/pii/S2210832714000088},
volume = {12},
year = {2016}
}
@inproceedings{Al2013,
abstract = {Migrating software product variants which are deemed similar into a product line is a challenging task with main impact in software reengineering. To exploit existing software variants to build a software product line (SPL), the ﬁrst step is to mine the feature model of this SPL which involves extracting common and optional features. Thus, we propose, in this paper, a new approach to mine features from the object-oriented source code of software variants by using lexical and structural similarity. To validate our approach, we applied it on ArgoUML, Health Watcher and Mobile Media software. The results of this evaluation showed that most of the features were identiﬁed.},
author = {Al-msie'deen, R. and Seriai, A.-D. and Huchard, Marianne and Urtado, Christelle and Vauttier, Sylvain},
booktitle = {2013 IEEE 14th International Conference on Information Reuse {\&} Integration (IRI)},
doi = {10.1109/IRI.2013.6642522},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/06642522.pdf:pdf},
isbn = {978-1-4799-1050-2},
keywords = {Code Dependencies,Computer Science/Software Engineering,Feature Mining,Formal Concept Analysis,Informatique/G{\'{e}}nie Logiciel,Latent Semantic Indexing,Software Product Line,Software Product Variants,Structural Similarity,naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
month = {aug},
pages = {586--593},
publisher = {IEEE},
title = {{Mining features from the object-oriented source code of software variants by combining lexical and structural similarity}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6642522},
year = {2013}
}
@article{Fan2014,
abstract = {This paper presents a data mining (DM) based approach to developing ensemble models for predicting next-day energy consumption and peak power demand, with the aim of improving the prediction accuracy. This approach mainly consists of three steps. Firstly, outlier detection, which merges feature extraction, clustering analysis, and the generalized extreme studentized deviate (GESD), is performed to remove the abnormal daily energy consumption profiles. Secondly, the recursive feature elimination (RFE), an embedded variable selection method, is applied to select the optimal inputs to the base prediction models developed separately using eight popular predictive algorithms. The parameters of each model are then obtained through leave-group-out cross validation (LGOCV). Finally, the ensemble model is developed and the weights of the eight predictive models are optimized using genetic algorithm (GA). The approach is adopted to analyze the large energy consumption data of the tallest building in Hong Kong. The prediction accuracies of the ensemble models measured by mean absolute percentage error (MAPE) are 2.32{\%} and 2.85{\%} for the next-day energy consumption and peak power demand respectively, which are evidently higher than those of individual base models. The results also show that the outlier detection method is effective in identifying the abnormal daily energy consumption profiles. The RFE process can significantly reduce the computation load while enhancing the model performance. The ensemble models are valuable for developing strategies of fault detection and diagnosis, operation optimization and interactions between buildings and smart grid.},
author = {Fan, Cheng and Xiao, Fu and Wang, Shengwei},
doi = {10.1016/j.apenergy.2014.04.016},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Fan, Xiao, Wang - 2014 - Development of prediction models for next-day building energy consumption and peak power demand using data mini.pdf:pdf},
issn = {03062619},
journal = {Applied Energy},
keywords = {Building energy prediction,Clustering analysis,Data mining,Ensemble model,Feature extraction,Recursive feature elimination,naranja},
mendeley-groups = {estado del arte,con doi,propuesta,informe},
mendeley-tags = {naranja},
month = {aug},
pages = {1--10},
title = {{Development of prediction models for next-day building energy consumption and peak power demand using data mining techniques}},
url = {http://www.sciencedirect.com/science/article/pii/S0306261914003596 http://linkinghub.elsevier.com/retrieve/pii/S0306261914003596},
volume = {127},
year = {2014}
}
@inproceedings{Mazo2015,
abstract = {This paper presents the new release of VariaMos, a Java-based tool for defining variability modeling languages, modeling (dynamic) product lines and cyber-physical self-adaptive systems, and supporting automated verification, analysis, configuration and simulation of these models. In particular, we describe the characteristics of this new version regarding its first release: (1) the capability to create languages for modeling systems with variability, even with different views; (2) the capability to use the created language to model (dynamic) product lines; (3) the capability to analyze and configure these models according to the changing context and requirements; and (4) the capability to execute them over several simulation scenarios. Finally, we show how to use VariaMos with an example, and we compare it with other tools found in the literature.},
address = {New York, New York, USA},
author = {Mazo, Ra{\'{u}}l and Mu{\~{n}}oz-Fern{\'{a}}ndez, Juan C and Rinc{\'{o}}n, Luisa and Salinesi, Camille and Tamura, Gabriel},
booktitle = {Proceedings of the 19th International Conference on Software Product Line - SPLC '15},
doi = {10.1145/2791060.2791103},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/mazo2015.pdf:pdf},
isbn = {9781450336130},
keywords = {2,3,4,and constraint,constraints,dopler,dynamic product line,goals,models,ovm,product line engineering,purpura,simulation,tool,variability},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura},
pages = {374--379},
publisher = {ACM Press},
title = {{VariaMos}},
url = {http://dl.acm.org/citation.cfm?doid=2791060.2791103},
year = {2015}
}
@inproceedings{Zhou2015b,
abstract = {A feature model is able to identify commonality and variability within a product line, helping stakeholders configure product variants and seize opportunities for reuse. However, no direct customer preference information is incorporated in the feature model when it comes to the question-how many product variants are needed in order to satisfy individual customer needs. This paper proposes to mine customer preference information for individual product features by sentiment analysis of online product reviews. The features commented by the users of a product are used to augment a simple feature model predefined with customer opinionated preference information. In such a way, the customer preference information is considered as one attribute of the features in the model, helping designers make informed decisions when trading off between commonality and variability of a product line. Finally, we present a Kindle Fire tablet case study to demonstrate the proposed method.},
author = {Zhou, F. and Jiao, R. J.},
booktitle = {2015 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)},
doi = {10.1109/IEEM.2015.7385935},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/zhou2015.pdf:pdf},
isbn = {978-1-4673-8066-9},
keywords = {Analytical models,Consumer electronics,Electronic publishing,Feature extraction,Feature model,Fires,High definition video,Kindle fire tablet case study,Sentiment analysis,commonality,customer opinionated preference information,customer preference,customer preference information,customer services,feature model augmentation,individual customer need,individual product feature,naranja,online product review,product line planning,product variant,production planning,rojo,sentiment analysis,variability},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,rojo},
month = {dec},
pages = {1689--1693},
publisher = {IEEE},
title = {{Feature model augmentation with sentiment analysis for product line planning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7385935},
year = {2015}
}
@article{Liao2008,
abstract = {Retailing consists of the final activities and steps needed to place a product in the hands of the consumer or to provide services to the consumer. In fact, retailing is actually the last step in a supply chain that may stretch from Europe or Asia to the customer's hometown. Therefore, any firm that sells a product or provides a service to the final consumer is performing the retailing function. On the other hand, product line extension, which adds depth to an existing product line by introducing new products in the same product category, can give customers greater choice and help to protect the firm from flanking attack by a competitor. In addition, a product line extension is marketed under the same general brand as a previous item or items. Thus, to distinguish the brand extension from the other item(s) under the primary brand, the retailer can either add secondary brand identification or add a generic brand. This paper investigates product line and brand extension issues in the Taiwan branch of a leading international retailing company, Carrefour, which is a hypermarket retailer. This paper develops a relational database and proposes Apriori algorithm and K-means as methodologies for association rule and cluster analysis for data mining, which is then implemented to mine customer knowledge from household customers. Knowledge extraction by data mining results is illustrated as knowledge patterns/rules and clusters in order to propose suggestions and solutions to the case firm for product line and brand extensions and knowledge management. ?? 2007.},
author = {Liao, Shu-Hsien and Chen, Chyuan-Meei and Wu, Chung-Hsin},
doi = {10.1016/j.eswa.2007.01.036},
file = {:Users/josepplloo/Documents/primer inmersion/minig customer knowledge.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Association rules,Brand extension,Cluster analysis,Data mining,Knowledge extraction,Product line extension,Retailing,naranja,purpura},
mendeley-groups = {con doi,informe},
mendeley-tags = {naranja,purpura},
month = {apr},
number = {3},
pages = {1763--1776},
title = {{Mining customer knowledge for product line and brand extension in retailing}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S095741740700053X},
volume = {34},
year = {2008}
}
@article{Mangalova2014a,
abstract = {The paper deals with a modeling procedure which aims to predict the power output of wind farm electricity generators. The following modeling steps are proposed: factor selection, raw data pretreatment, model evaluation and optimization. Both heuristic and formal methods are combined to construct the model. The basic modeling approach here is the k-nearest neighbors method.},
author = {Mangalova, E. and Agafonov, E.},
doi = {10.1016/j.ijforecast.2013.07.008},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Mangalova, Agafonov - 2014 - Wind power forecasting using the mmlmath altimg=si14.gif display=inline overflow=scroll xmlnsxocs=httpwww.e.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Cross-validation,Data mining,Energy forecasting*,Feature selection,Forecasting competitions*,Nonparametric models,Regression tree,rojo},
mendeley-groups = {con doi,informe},
mendeley-tags = {rojo},
month = {apr},
number = {2},
pages = {402--406},
title = {{Wind power forecasting using the {\textless}mml:math altimg="si14.gif" display="inline" overflow="scroll" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xs="http://www.w3.org/2001/XMLSchema" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http}},
url = {http://www.sciencedirect.com/science/article/pii/S0169207013000848 http://linkinghub.elsevier.com/retrieve/pii/S0169207013000848},
volume = {30},
year = {2014}
}
@book{Izenman2006,
abstract = {Review From the reviews: .,."There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006 "The reader sees not only how measure theory is used to develop probability theory, but also how probability theory is used in applications. a The discourse is delivered in a theorem proof format and thus is better suited for classroom a . The authors prose is generally well thought out a . will make an attractive choice for a two-semester course on measure and probability, or as a second course for students with a semester of measure or probability theory under their belt." (Peter C. Kiessler, Journal of the American Statistical Association, Vol. 102 (479), 2007) "The book is a well written self-contained textbook on measure and probability theory. It consists of 18 chapters. Every chapter contains many well chosen examples and ends with several problems related to the earlier developed theory (some with hints). a At the very end of the book there is an appendix collecting necessary facts from set theory, calculus and metric spaces. The authors suggest a few possibilities on how to use their book." (Kazimierz Musial, Zentralblatt MATH, Vol. 1125 (2), 2008) "The title of the book consists of the names of its two basic parts. The booka (TM)s third part is comprised of some special topics from probability theory. a The authors suggest using the book intwo-semester graduate programs in statistics or a one-semester seminar on special topics. The material of the book is standard a is clear, comprehensive and a {\~{}}without being intimidatinga (TM)." (Rimas NorvaiAa, Mathematical Reviews, Issue 2007 f) Product Description This is a graduate level textbook on measure theory and probability theory. The book can be used as a text for a two semester sequence of courses in measure theory and probability theory, with an option to include supplemental material on stochastic processes and special topics. It is intended primarily for first year Ph.D. students in mathematics and statistics although mathematically advanced students from engineering and economics would also find the book useful. Prerequisites are kept to the minimal level of an understanding of basic real analysis concepts such as limits, continuity, differentiability, Riemann integration, and convergence of sequences and series. A review of this material is included in the appendix. The book starts with an informal introduction that provides some heuristics into the abstract concepts of measure and integration theory, which are then rigorously developed. The first part of the book can be used for a standard real analysis course for both mathematics and statistics Ph.D. students as it provides full coverage of topics such as the construction of Lebesgue-Stieltjes measures on real line and Euclidean spaces, the basic convergence theorems, L p spaces, signed measures, Radon-Nikodym theorem, Lebesgue's decomposition theorem and the fundamental theorem of Lebesgue integration on R, product spaces and product measures, and Fubini-Tonelli theorems. It also provides an elementary introduction to Banach and Hilbert spaces, convolutions, Fourier series and Fourier and Plancherel transforms. Thus part I would be particularly useful for students in a typical Statistics Ph.D. program if a separate course on real analysis is not a standard requirement. Part II (chapters 6-13) provides full coverage of standard graduate level probability theory. It starts with Kolmogorov's probability model and Kolmogorov's existence theorem. It then treats thoroughly the laws of large numbers including renewal theory and ergodic theorems with applications and then weak convergence of probability distributions, characteristic functions, the Levy-Cramer continuity theorem and the central limit theorem as well as stable laws. It ends with conditional expectations and conditional probability, and an introduction to the theory of discrete time martingales. Part III (chapters 14-18) provides a modest coverage of discrete time Markov chains with countable and general state spaces, MCMC, continuous time discrete space jump Markov processes, Brownian motion, mixing sequences, bootstrap methods, and branching processes. It could be used for a topics/seminar course or as an introduction to stochastic processes. From the reviews: "...There are interesting and non-standard topics that are not usually included in a first course in measture-theoretic probability including Markov Chains and MCMC, the bootstrap, limit theorems for martingales and mixing sequences, Brownian motion and Markov processes. The material is well-suported with many end-of-chapter problems." D.L. McLeish for Short Book Reviews of the ISI, December 2006},
address = {New York, NY},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Izenman, Alan J.},
booktitle = {Performance Evaluation},
doi = {10.1007/978-0-387-78189-1},
eprint = {arXiv:1011.1669v3},
file = {:Users/josepplloo/Documents/primer inmersion/libros de aprendizaje estadistico/(Springer Texts in Statistics) Alan J. Izenman (auth.)-Modern Multivariate Statistical Techniques{\_} Regression, Classification, and Manifold Learning-Springer-Verlag New York (2008).pdf:pdf},
isbn = {978-0-387-78188-4},
issn = {01665316},
keywords = {rojo},
mendeley-groups = {estado del arte,libros de dm,con doi,propuesta,informe},
mendeley-tags = {rojo},
month = {oct},
number = {9-12},
pages = {856--875},
pmid = {10911016},
publisher = {Springer New York},
series = {Springer Texts in Statistics},
title = {{Modern Multivariate Statistical Techniques}},
url = {http://books.google.com/books?id=9tv0taI8l6YC http://linkinghub.elsevier.com/retrieve/pii/S0166531607000570 http://link.springer.com/10.1007/978-0-387-78189-1},
volume = {64},
year = {2008}
}
@article{Manco2016,
author = {Manco, Giuseppe and Rullo, Pasquale and Gallucci, Lorenzo and Paturzo, Mirko},
doi = {10.1016/j.eswa.2016.04.022},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/1-s2.0-S0957417416301889-main.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Business a,Data mining,Knowledge Discovery process,azul,gris,knowledge discovery process},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {azul,gris},
month = {oct},
pages = {145--164},
publisher = {Elsevier Ltd},
title = {{Rialto: A Knowledge Discovery suite for data analysis}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417416301889},
volume = {59},
year = {2016}
}
@article{Jiao2005,
abstract = {It has been well recognized that product portfolio planning has far-reaching impact on the company's business success in competition. In general, product portfolio planning involves two main stages, namely portfolio identification and portfolio evaluation and selection. The former aims to capture and understand customer needs effectively and accordingly to transform them into specifications of product offerings. The latter concerns how to determine an optimal configuration of these identified offerings with the objective of achieving best profit performance. Current research and industrial practice have mainly focused on the economic justification of a given product portfolio, whereas the portfolio identification issue has been received only limited attention. This article intends to develop explicit decision support to improve product portfolio identification by efficient knowledge discovery from past sales and product records. As one of the important applications of data mining, association rule mining lends itself to the discovery of useful patterns associated with requirement analysis enacted among customers, marketing folks, and designers. An association rule mining system (ARMS) is proposed for effective product portfolio identification. Based on a scrutiny into the product definition process, the article studies the fundamental issues underlying product portfolio identification. The ARMS differentiates the customer needs from functional requirements involved in the respective customer and functional domains. Product portfolio identification entails the identification of functional requirement clusters in conjunction with the mappings from customer needs to these clusters. While clusters of functional requirements are identified based on fuzzy clustering analysis, the mapping mechanism between the customer and functional domains is incarnated in association rules. The ARMS architecture and implementation issues are discussed in detail. An application of the proposed methodology and system in a consumer electronics company to generate a vibration motor portfolio for mobile phones is also presented.},
author = {Jiao, Jianxin and Zhang, Yiyang},
doi = {10.1016/j.cad.2004.05.006},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/10.1016@j.cad.2004.05.006.pdf:pdf},
isbn = {6567904143},
issn = {00104485},
journal = {Computer-Aided Design},
keywords = {association rules,customer satisfaction,data mining,mass customization,product,product portfolio,purpura,requirement management,rojo,variety},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura,rojo},
month = {feb},
number = {2},
pages = {149--172},
title = {{Product portfolio identification based on association rule mining}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0010448504001010},
volume = {37},
year = {2005}
}
@article{Quevedo2010,
abstract = {This paper presents a signal analysis methodology to validate (detect) and reconstruct the missing and false data of a large set of flow meters in the telecontrol system of a water distribution network. The proposed methodology is based on two time-scale forecasting models: a daily model based on a ARIMA time series, while the 10-min model is based on distributing the daily flow using a 10-min demand pattern. The demand patterns have been determined using two methods: correlation analysis and an unsupervised fuzzy logic classification, named LAMDA algorithm. Finally, the proposed methodology has been applied to the Barcelona water distribution network, providing very good results. {\textcopyright} 2010 Elsevier Ltd.},
author = {Quevedo, J. and Puig, V. and Cembrano, G. and Blanch, J. and Aguilar, J. and Saporta, D. and Benito, G. and Hedo, M. and Molina, A.},
doi = {10.1016/j.conengprac.2010.03.003},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/Validation and reconstruction of flow meter data in the Barcelona water distribution network.pdf:pdf},
issn = {09670661},
journal = {Control Engineering Practice},
keywords = {Fault detection,Flow meter,Fuzzy logic classifier,Sensor failure,Telecontrol system,Water distribution network},
mendeley-groups = {estado del arte,con doi,informe},
month = {jun},
number = {6},
pages = {640--651},
publisher = {Elsevier},
title = {{Validation and reconstruction of flow meter data in the Barcelona water distribution network}},
url = {http://dx.doi.org/10.1016/j.conengprac.2010.03.003 http://linkinghub.elsevier.com/retrieve/pii/S0967066110000791},
volume = {18},
year = {2010}
}
@article{Zhang2014,
abstract = {A key issue of quantitative investment (QI) product design is how to select representative features for stock prediction. However, existing stock prediction models adopt feature selection algorithms that rely on correlation analysis. This paper is the first to apply observational data-based causal analysis to stock prediction. Causalities represent direct influences between various stock features (important for stock analysis), while correlations cannot distinguish direct influences from indirect ones. This study proposes the causal feature selection (CFS) algorithm to select more representative features for better stock prediction modeling. CFS first identifies causalities between variables and then, based on the results, generates a feature subset. Based on 13-year data from the Shanghai Stock Exchanges, comparative experiments were conducted between CFS and three well-known feature selection algorithms, namely, principal component analysis (PCA), decision trees (DT; CART), and the least absolute shrinkage and selection operator (LASSO). CFS performs best in terms of accuracy and precision in most cases when combined with each of the seven baseline models, and identifies 18 important consistent features. In conclusion, CFS has considerable potential to improve the development of QI product.},
author = {Zhang, Xiangzhou and Hu, Yong and Xie, Kang and Wang, Shouyang and Ngai, E.W.T. and Liu, Mei},
doi = {10.1016/j.neucom.2014.01.057},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2014 - A causal feature selection algorithm for stock prediction modeling.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Causal discovery,Data mining,Feature selection,Stock prediction,V-structure,naranja,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,rojo},
month = {oct},
pages = {48--59},
title = {{A causal feature selection algorithm for stock prediction modeling}},
url = {http://www.sciencedirect.com/science/article/pii/S0925231214005359 http://linkinghub.elsevier.com/retrieve/pii/S0925231214005359},
volume = {142},
year = {2014}
}
@inproceedings{Santos2015,
address = {New York, New York, USA},
author = {Santos, Alcemir Rodrigues and de Oliveira, Raphael Pereira and de Almeida, Eduardo Santana},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering - EASE '15},
doi = {10.1145/2745802.2745806},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/santos2015.pdf:pdf},
isbn = {9781450333504},
keywords = {consistency checking,literature review,mapping study,naranja,software product line engineering},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
number = {Section 3},
pages = {1--14},
publisher = {ACM Press},
title = {{Strategies for consistency checking on software product lines}},
url = {http://dl.acm.org/citation.cfm?doid=2745802.2745806},
year = {2015}
}
@article{Asadi2014,
abstract = {Context A software product line is a family of software systems that share some common features but also have significant variabilities. A feature model is a variability modeling artifact, which represents differences among software products with respect to the variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by binding the variation points in the feature model (called configuration process) and by instantiating the reference model. Objective In this work we address the feature model configuration problem and propose a framework to automatically select suitable features that satisfy both the functional and non-functional preferences and constraints of stakeholders. Additionally, interdependencies between various non-functional properties are taken into account in the framework. Method The proposed framework combines Analytical Hierarchy Process (AHP) and Fuzzy Cognitive Maps (FCM) to compute the non-functional properties weights based on stakeholders' preferences and interdependencies between non-functional properties. Afterwards, Hierarchical Task Network (HTN) planning is applied to find the optimal feature model configuration. Result Our approach improves state-of-art of feature model configuration by considering positive or negative impacts of the features on non-functional properties, the stakeholders' preferences, and non-functional interdependencies. The approach presented in this paper extends earlier work presented in [1] from several distinct perspectives including mechanisms handling interdependencies between non-functional properties, proposing a novel tooling architecture, and offering visualization and interaction techniques for representing functional and non-functional aspects of feature models. Conclusion our experiments show the scalability of our configuration approach when considering both functional and non-functional requirements of stakeholders. ?? 2014 Elsevier B.V. All rights reserved.},
author = {Asadi, Mohsen and Soltani, Samaneh and Gasevic, Dragan and Hatala, Marek and Bagheri, Ebrahim},
doi = {10.1016/j.infsof.2014.03.005},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/asadi2014.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Feature model configuration,Non-functional interdependencies,Software product lines,Stakeholders' preferences,gris,naranja,rojo},
mendeley-groups = {estado del arte,con doi,propuesta,informe},
mendeley-tags = {gris,naranja,rojo},
month = {sep},
number = {9},
pages = {1144--1165},
publisher = {Elsevier B.V.},
title = {{Toward automated feature model configuration with optimizing non-functional requirements}},
url = {http://dx.doi.org/10.1016/j.infsof.2014.03.005 http://linkinghub.elsevier.com/retrieve/pii/S0950584914000640},
volume = {56},
year = {2014}
}
@article{Ghahramani2015,
abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
author = {Ghahramani, Zoubin},
doi = {10.1038/nature14541},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/nature/nature14541.pdf:pdf},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
keywords = {naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
month = {may},
number = {7553},
pages = {452--459},
pmid = {26017444},
title = {{Probabilistic machine learning and artificial intelligence}},
url = {http://dx.doi.org/10.1038/nature14541 http://www.nature.com/doifinder/10.1038/nature14541},
volume = {521},
year = {2015}
}
@book{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
address = {New York, NY},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
booktitle = {Elements},
doi = {10.1007/b94608},
file = {:Users/josepplloo/Downloads/Hastie ESLII(2008).pdf:pdf},
isbn = {978-0-387-84857-0},
issn = {03436993},
keywords = {verde},
mendeley-groups = {libros de dm,con doi,propuesta,informe},
mendeley-tags = {verde},
pages = {337--387},
pmid = {15512507},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/10.1007/b94608},
volume = {1},
year = {2009}
}
@article{Benavides2010,
author = {Benavides, David and Segura, Sergio and Ruiz-Cort{\'{e}}s, Antonio},
chapter = {gris},
doi = {10.1016/j.is.2010.01.001},
file = {:Users/josepplloo/Downloads/benavides10-is-Features Models.pdf:pdf},
issn = {03064379},
journal = {Information Systems},
keywords = {automated analyses,feature models,literature review,software product lines},
mendeley-groups = {estado del arte,con doi,propuesta,informe},
month = {sep},
number = {6},
pages = {615--636},
title = {{Automated analysis of feature models 20 years later: A literature review}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0306437910000025},
volume = {35},
year = {2010}
}
@inproceedings{Huang2013,
abstract = {The mining software repositories (MSR) analyze data stored in software repositories and discover meaningful information to support software development. However, MSR is complex due to conducting large scale data collection with various repositories. To help practitioners perform MSR analysis, one possible way is to apply the approaches of software product line (SPL) to the MSR domain to understand variability and commonality for the domain, and to construct domain specific languages (DSLs) because DSLs have high readability to reduce the complexity of the procedure of MSR. In this paper, we construct a SQL-based DSL to support MSR and provide a systematic approach to conduct Feature-Oriented Domain Analysis (FODA) for MSR towards the construction of the DSL. We provide the syntax of the DSL and explain how to locate language elements of the DSL to the four-layer structure used in FODA. {\textcopyright} 2013 IEEE.},
author = {Huang, Changyun and Yamashita, Kazuhiro and Kamei, Yasutaka and Hisazumi, Kenji and Ubayashi, Naoyasu},
booktitle = {2013 4th International Workshop on Product LinE Approaches in Software Engineering (PLEASE)},
doi = {10.1109/PLEASE.2013.6608663},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/06608663.pdf:pdf},
isbn = {978-1-4673-6449-2},
keywords = {naranja,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,rojo},
month = {may},
pages = {41--44},
publisher = {IEEE},
title = {{Domain analysis for mining software repositories: Towards feature-based DSL construction}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6608663{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6608663 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6608663},
year = {2013}
}
@article{Lee2014,
author = {Lee, Jaejoon and Kang, Kyo C. and Sawyer, Pete and Lee, Hyesun},
doi = {10.1007/s00766-013-0183-6},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/springer/98837686.pdf:pdf},
issn = {0947-3602},
journal = {Requirements Engineering},
keywords = {Feature modeling,Feature modeling viewpoints,Feature space,Goal modeling,Product line requirements engineering,azul,naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {azul,naranja},
month = {nov},
number = {4},
pages = {377--395},
title = {{A holistic approach to feature modeling for product line requirements engineering}},
url = {http://link.springer.com/10.1007/s00766-013-0183-6},
volume = {19},
year = {2014}
}
@article{Maldonado2015,
abstract = {Churn prediction is an important application of classification models that identify those customers most likely to attrite based on their respective characteristics described by e.g. socio-demographic and behavioral variables. Since nowadays more and more of such features are captured and stored in the respective computational systems, an appropriate handling of the resulting information overload becomes a highly relevant issue when it comes to build customer retention systems based on churn prediction models. As a consequence, feature selection is an important step of the classifier construction process. Most feature selection techniques; however, are based on statistically inspired validation criteria, which not necessarily lead to models that optimize goals specified by the respective organization. In this paper we propose a profit-driven approach for classifier construction and simultaneous variable selection based on support vector machines. Experimental results show that our models outperform conventional techniques for feature selection achieving superior performance with respect to business-related goals.},
author = {Maldonado, Sebasti{\'{a}}n and Flores, {\'{A}}lvaro and Verbraken, Thomas and Baesens, Bart and Weber, Richard},
doi = {10.1016/j.asoc.2015.05.058},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Maldonado et al. - 2015 - Profit-based feature selection using support vector machines – General framework and an application for cust.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing},
keywords = {Churn prediction,Customer retention,Data mining,Feature selection,Maximum profit,Support vector machines,naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
month = {oct},
pages = {740--748},
title = {{Profit-based feature selection using support vector machines – General framework and an application for customer retention}},
url = {http://www.sciencedirect.com/science/article/pii/S1568494615004093 http://linkinghub.elsevier.com/retrieve/pii/S1568494615004093},
volume = {35},
year = {2015}
}
@inproceedings{Czarnecki2008,
abstract = {We present probabilistic feature models (PFMs) and illustrate their use by discussing modeling, mining and interactive configuration. PFMs are formalized as a set of formulas in a certain probabilistic logic. Such formulas can express both hard and soft constraints and have a well defined semantics by denoting a set of joint probability distributions over features. We show how PFMs can be mined from a given set of feature configurations using data mining techniques. Finally, we demonstrate how PFMs can be used in configuration in order to provide automated support for choice propagation based on both hard and soft constraints. We believe that these results constitute solid foundations for the construction of reverse engineering tools for software product lines and configurators using soft constraints.},
author = {Czarnecki, Krzysztof and She, Steven and Wasowski, Andrzej},
booktitle = {2008 12th International Software Product Line Conference},
doi = {10.1109/SPLC.2008.49},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/czarnecki2008.pdf:pdf},
isbn = {978-0-7695-3303-2},
keywords = {gris,purpura},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris,purpura},
month = {sep},
pages = {22--31},
publisher = {IEEE},
title = {{Sample Spaces and Feature Models: There and Back Again}},
url = {http://ieeexplore.ieee.org/document/4626837/},
year = {2008}
}
@inproceedings{Mu2009,
abstract = {The acquisition of requirements assets are important in software product line (SPL) engineering for it help enhancing the effectiveness of reuse. Traditional methods are heavily based on manual effort. This appears to be a barrier for many organizations which tend to launch a SPL. In this paper, we propose an approach to extract functional requirements by analyzing text-based software requirements specifications (SRSs). We analyze the linguistic characterization of SRSs. According to it we define extended functional requirements framework (EFRF) which consists of 10 semantic cases, then we generate converting rules. We introduce an NLP (natural language process) approach to build EFRFs from documents based on the concept of EFRF and the converting rules. The extracted EFRFs are suitable for expression and modeling of functional requirements variability. We apply our method to an auto-marker software product line. The result shows the approach has high accuracy and efficiency, and the approach is readily scalable and extensible.},
author = {Mu, Yunhe and Wang, Yinglin and Guo, Jianmei},
booktitle = {2009 International Conference on Information and Multimedia Technology},
doi = {10.1109/ICIMT.2009.47},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/05381217.pdf:pdf},
isbn = {978-1-4244-5383-2},
keywords = {Functional requirement,Information extract,Product line,Rule-based,Variability model,naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
pages = {194--198},
publisher = {IEEE},
title = {{Extracting Software Functional Requirements from Free Text Documents}},
url = {http://ieeexplore.ieee.org/document/5381217/},
year = {2009}
}
@article{Wang2012,
abstract = {In an imbalanced dataset, the positive and negative classes can be quite different in both size and distribution. This degrades the performance of many feature extraction methods and classifiers. This paper proposes a method for extracting minimum positive and maximum negative features (in terms of absolute value) for imbalanced binary classification. This paper develops two models to yield the feature extractors. Model 1 first generates a set of candidate extractors that can minimize the positive features to be zero, and then chooses the ones among these candidates that can maximize the negative features. Model 2 first generates a set of candidate extractors that can maximize the negative features, and then chooses the ones that can minimize the positive features. Compared with the traditional feature extraction methods and classifiers, the proposed models are less likely affected by the imbalance of the dataset. Experimental results show that these models can perform well when the positive class and negative class are imbalanced in both size and distribution. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
author = {Wang, Jinghua and You, Jane and Li, Qin and Xu, Yong},
doi = {10.1016/j.patcog.2011.09.004},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/1-s2.0-S0031320311003827-main.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Feature subspace extraction,Imbalanced binary classification,Maximum negative feature,Minimum positive feature,Pattern classification,purpura,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura,rojo},
month = {mar},
number = {3},
pages = {1136--1145},
publisher = {Elsevier},
title = {{Extract minimum positive and maximum negative features for imbalanced binary classification}},
url = {http://dx.doi.org/10.1016/j.patcog.2011.09.004 http://linkinghub.elsevier.com/retrieve/pii/S0031320311003827},
volume = {45},
year = {2012}
}
@article{Reuling2015,
abstract = {Testing every member of a product line individually is of-ten impracticable due to large number of possible product configurations. Thus, feature models are frequently used to generate samples, i.e., subsets of product configurations under test. Besides the extensively studied combinatorial interaction testing (CIT) approach for coverage-driven sam-ple generation, only few approaches exist so far adopting mutation testing to emulate faults in feature models to be detected by a sample. In this paper, we present a mutation-based sampling framework for fault-based product-line test-ing. We define a comprehensive catalog of atomic mutation operators on the graphical representation of feature models. This way, we are able (1) to also define complex mutation operators emulating more subtle faults, and (2) to classify operators semantically, e.g., to avoid redundant and equiv-alent mutants. We further introduce similarity-based mu-tant selection and higher order mutation strategies to reduce testing efforts. Our implementation is based on the graph transformation engine Henshin and is evaluated concerning effectiveness/efficiency trade-offs.},
author = {Reuling, Dennis and B{\"{u}}rdek, Johannes and Rot{\"{a}}rmel, Serge and Lochau, Malte and Kelter, Udo},
doi = {10.1145/2791060.2791074},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/reuling2015.pdf:pdf},
isbn = {9781450336130},
journal = {Proceedings of the 19th International Conference on Software Product Line},
keywords = {combinatorial interaction testing,gris,mutation testing},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris},
pages = {131--140},
title = {{Fault-based product-line testing: effective sample generation based on feature-diagram mutation}},
year = {2015}
}
@article{Sajadfar2015,
abstract = {This paper presents an informatics framework to apply feature-based engineering concept for cost estimation supported with data mining algorithms. The purpose of this research work is to provide a practical procedure for more accurate cost estimation by using the commonly available manufacturing process data associated with ERP systems. The proposed method combines linear regression and data-mining techniques, leverages the unique strengths of the both, and creates a mechanism to discover cost features. The final estimation function takes the user's confidence level over each member technique into consideration such that the application of the method can phase in gradually in reality by building up the data mining capability. A case study demonstrates the proposed framework and compares the results from empirical cost prediction and data mining. The case study results indicate that the combined method is flexible and promising for determining the costs of the example welding features. With the result comparison between the empirical prediction and five different data mining algorithms, the ANN algorithm shows to be the most accurate for welding operations.},
author = {Sajadfar, Narges and Ma, Yongsheng},
doi = {10.1016/j.aei.2015.06.001},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Sajadfar, Ma - 2015 - A hybrid cost estimation framework based on feature-oriented data mining approach.pdf:pdf},
issn = {14740346},
journal = {Advanced Engineering Informatics},
keywords = {Cost estimation,Data mining,ERP,Feature modeling,Welding feature,gris},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris},
month = {aug},
number = {3},
pages = {633--647},
title = {{A hybrid cost estimation framework based on feature-oriented data mining approach}},
url = {http://www.sciencedirect.com/science/article/pii/S1474034615000610 http://linkinghub.elsevier.com/retrieve/pii/S1474034615000610},
volume = {29},
year = {2015}
}
@inproceedings{Chen2014a,
author = {Chen, Qian and Zhu, Wenhao and Ju, Chaoyou and Zhang, Wu},
booktitle = {2014 10th International Conference on Natural Computation (ICNC)},
doi = {10.1109/ICNC.2014.6975936},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/chen2014.pdf:pdf},
isbn = {978-1-4799-5151-2},
keywords = {Cross domain,Information extraction,Multi-level feature model,naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
month = {aug},
pages = {780--784},
publisher = {IEEE},
title = {{Cross domain web information extraction with multi-level feature model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6975936},
year = {2014}
}
@inproceedings{TerBeek2015,
address = {New York, New York, USA},
author = {ter Beek, M. H. and Legay, A. and Lafuente, A. Lluch and Vandin, A.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line - SPLC '15},
doi = {10.1145/2791060.2791087},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/dspl20152.pdf:pdf},
isbn = {9781450336130},
keywords = {gris},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris},
pages = {11--15},
publisher = {ACM Press},
title = {{Statistical analysis of probabilistic models of software product lines with quantitative constraints}},
url = {http://dl.acm.org/citation.cfm?doid=2791060.2791087},
year = {2015}
}
@techreport{Ardila2015,
address = {Bogot{\'{a}}},
author = {Ardila, Diego Silva},
file = {:Users/josepplloo/Documents/primer inmersion/bol{\_}empresas{\_}2013.pdf:pdf},
institution = {DANE},
mendeley-groups = {estado del arte,propuesta,informe},
pages = {1--64},
title = {{Indicadores B{\'{a}}sicos de Tenencia y Uso de Tecnolog{\'{i}}as de la Informaci{\'{o}}n y Comunicaci{\'{o}}n en empresas 2013 Cifras Definitivas}},
year = {2015}
}
@inproceedings{Zhang2014b,
abstract = {As a Software Product Line (SPL) evolves with increasing number of variant features and feature values, the feature correlations become extremely intricate. However, these correlations are often incompletely documented (e.g., In feature models) so that most features can only be configured manually. In order to make product configuration processes more efficient, we present an approach to extracting complex feature correlations from existing product configurations using association mining techniques. Then these correlations are pruned and prioritized in order to minimize the effort of correlation validation. Our approach is conducted on an industrial SPL with 100 product configurations across 480 features. While 80 out of the 100 configurations are used as training data to automatically extract 4834 complex feature correlations, the rest 20 configurations are used as test data to evaluate the improvement potential of configuration efficiency. In the end, avg. 25{\%} features in each of the 20 products can be configured automatically.},
author = {Zhang, Bo and Becker, Martin},
booktitle = {2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications},
doi = {10.1109/SEAA.2014.34},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/06928830.pdf:pdf},
isbn = {978-1-4799-5795-8},
keywords = {feature correlation mining,naranja,product line configuration improvement,variability reverse engineering},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
month = {aug},
pages = {320--327},
publisher = {IEEE},
title = {{Reverse Engineering Complex Feature Correlations for Product Line Configuration Improvement}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6928830},
year = {2014}
}
@inproceedings{Salinesi2009a,
abstract = {Product line models (PLM) are important artifacts in product line engineering. Due to their size and complexity, it is difficult to detect defects in PLMs. The challenge is however important: any error in a PLM will inevitably impact configuration, generating issues such as incorrect product models, inconsistent architectures, poor reuse, difficulty to customize products, etc. Surveys on feature-based PLM verification approaches show that there are many verification criteria, that these criteria are defined in different ways, and that different ways of working are proposed to look for defect. The goal of this paper is to systematize PLM verification. Based on our literature review, we propose a list of 23 verification criteria that we think cover those available in the literature.},
author = {Salinesi, Camille and Rolland, Colette and Diaz, Daniel and Mazo, Ra{\'{u}}l},
booktitle = {2009 17th IEEE International Requirements Engineering Conference},
doi = {10.1109/RE.2009.57},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/salinesi2009.pdf:pdf},
isbn = {978-0-7695-3761-0},
issn = {1090705X},
keywords = {gris},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris},
month = {aug},
pages = {385--386},
publisher = {IEEE},
title = {{Looking for Product Line Feature Models Defects: Towards a Systematic Classification of Verification Criteria}},
url = {http://ieeexplore.ieee.org/document/5328480/},
year = {2009}
}
@article{Bocca2016,
abstract = {Crop yield models can assist decision makers within any agro-industrial supply chain, even with regard to decisions that are unrelated to the crop production. Considering the characteristics of the mechanisms and data related to yield, data mining techniques are suitable candidates for modelling. The use of these techniques within a context with feature engineering, feature selection, and proper tuning can further improve performance beyond a simple replacement of multiple linear regression. To evaluate the impact of the different steps in the mentioned context, we evaluated sugarcane (Saccharum spp.) yield modelling with data obtained from a sugarcane mill. For a combination of six techniques, tuning, feature selection, and feature engineering, leading to 66 combinations, we assessed final model performance. Average performance across combinations resulted in a mean absolute error (MAE) of 6.42Mgha−1. Using different techniques led to a range of MAE from 4.57 to 8.80Mgha−1 on average. The best and worst performances for an individual model were MAEs of 4.11 and 9.00Mgha−1. Models with lower performance were close to simply predicting yield from the average yield for each number of cuts (MAE of 9.86Mgha−1). Tuning and feature engineering reduced the MAE on average by 1.17 and 0.64Mgha−1, respectively. Feature selection removed nearly 40{\%} of the features but increased the MAE by 0.19Mgha−1. The performance of models was improved by simple strategies such as decomposing weather attributes and detailing fertilisation. Evaluation of feature importance provided by the RReliefF feature selection algorithm was used to explain the performance gains. If empirical models are needed, they will rely on using advanced techniques, but they will need proper algorithm tuning and feature engineering to extract most of the information from datasets. Based on the results, we recommend following the presented workflow for the development of yield models.},
author = {Bocca, Felipe F. and Rodrigues, Luiz Henrique Antunes},
doi = {10.1016/j.compag.2016.08.015},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/1-s2.0-S0168169916306391-main.pdf:pdf},
issn = {01681699},
journal = {Computers and Electronics in Agriculture},
keywords = {artificial neural networks,boosted regression trees,naranja,purpura},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,purpura},
pages = {67--76},
publisher = {Elsevier B.V.},
title = {{The effect of tuning, feature engineering, and feature selection in data mining applied to rainfed sugarcane yield modelling}},
url = {http://dx.doi.org/10.1016/j.compag.2016.08.015},
volume = {128},
year = {2016}
}
@article{Wang2015,
abstract = {In this paper, we propose a data-driven network analysis based approach to predict individual choice set for customer choice modeling. Taking into account product associations and customer heterogeneity, we apply data analytics to mine existing data of customer choice set, which is then used to predict choice set for individual customers in a new choice modeling scenario. Product association network is constructed first to identify product communities based on existing data of customer choice sets, where links between products reflect the proximity or similarity of two products in customers' perceptual space. To account for customer heterogeneity, customers are classified into clusters (segments) based on their profile attributes and for each cluster the product consideration frequency is computed. For predicting choice sets, a probabilistic sampling approach is proposed integrating product associations, customer segments, and the link strengths in the product association network. In case studies, we first implement the approach using an example with simulated choice set data. The quality of predicted choice sets is examined by assessing the estimation bias of the developed choice model. We then demonstrate the proposed approach using actual survey data of vehicle choice, illustrating the impact of choice sets on the customer utility representation and the agreement between choice model and reality. From both examples, improved choice modeling results are consistently observed using the predicted choice sets, demonstrating the benefits of the proposed method for choice modeling.},
author = {Wang, Mingxian and Chen, Wei},
doi = {10.1115/1.4030160},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/asme/md{\_}137{\_}07{\_}071410.pdf:pdf},
issn = {1050-0472},
journal = {Journal of Mechanical Design},
keywords = {analytics,choice modeling,choice set,customer preference,data,network analysis,product association,purpura},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura},
month = {jul},
number = {7},
pages = {071409},
title = {{A Data-Driven Network Analysis Approach to Predicting Customer Choice Sets for Choice Modeling in Engineering Design}},
url = {http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?doi=10.1115/1.4030160},
volume = {137},
year = {2015}
}
@article{Soares2014,
abstract = {Software Product Lines (SPL) approach has been widely developed in academia and successfully applied in industry. Based on the selection of features, stakeholders can efficiently derive tailor-made programs satisfying different requirements. While SPL was very successful at building products based on identified features, achievements and preservation of many nonfunctional properties (NFPs) remain challenging. A knowledge how to deal with NFPs is still not fully obtained. In this paper, we present a systematic literature review of NFPs analysis for SPL products, focusing on runtime NFPs. The goal of the paper is twofold: (i) to present an holistic overview of SPL approaches that have been reported regarding the analysis of runtime NFPs, and (ii) to categorize NFPs treated in the scientific literature regarding development of SPLs. We analyzed 36 research papers, and identified that system performance attributes are typically the most considered. The results also aid future research studies in NFPs analysis by providing an unbiased view of the body of empirical evidence and by guiding future research directions.},
author = {Soares, Larissa Rocha and Potena, Pasqualina and Machado, Ivan Do Carmo and Crnkovic, Ivica and {De Almeida}, Eduardo Santana D},
doi = {10.1109/SEAA.2014.48},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/06928831.pdf:pdf},
isbn = {9781479957941},
journal = {Proceedings - 40th Euromicro Conference Series on Software Engineering and Advanced Applications, SEAA 2014},
keywords = {Non-functional Properties,Product Derivation,Software Product Lines,Systematic Literature Review,gris},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris},
pages = {328--335},
title = {{Analysis of non-functional properties in software product lines: A systematic review}},
year = {2014}
}
@inproceedings{Caruana2006,
abstract = {A number of supervised learning methods have been introduced in the last decade. Unfortunately, the last comprehensive empirical evaluation of supervised learning was the Statlog Project in the early 90's. We present a large-scale empirical comparison between ten supervised learning methods: SVMs, neural nets, logistic regression, naive bayes, memory-based learning, random forests, decision trees, bagged trees, boosted trees, and boosted stumps. We also examine the effect that calibrating the models via Platt Scaling and Isotonic Regression has on their performance. An important aspect of our study is the use of a variety of performance criteria to evaluate the learning methods.},
address = {New York, New York, USA},
author = {Caruana, Rich and Niculescu-Mizil, Alexandru},
booktitle = {Proceedings of the 23rd international conference on Machine learning - ICML '06},
doi = {10.1145/1143844.1143865},
file = {:Users/josepplloo/Documents/primer inmersion/libros de aprendizaje estadistico/caruana.icml06.pdf:pdf},
isbn = {1595933832},
issn = {1595933832},
keywords = {gris},
mendeley-groups = {primera,con doi,informe},
mendeley-tags = {gris},
number = {1},
pages = {161--168},
publisher = {ACM Press},
title = {{An empirical comparison of supervised learning algorithms}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143865},
volume = {C},
year = {2006}
}
@article{Zhou2015,
abstract = {Experts in finance and accounting select feature subset for corporate financial distress prediction according to their professional understanding of the characteristics of the features, while researchers in data mining often believe that data alone can tell everything and they use various mining techniques to search the feature subset without considering the financial and accounting meanings of the features. This paper investigates the performance of different financial distress prediction models with features selection approaches based on domain knowledge or data mining techniques. The empirical results show that there is no significant difference between the best classification performance of models with features selection guided by data mining techniques and that by domain knowledge. However, the combination of domain knowledge and genetic algorithm based features selection method can outperform unique domain knowledge and unique data mining based features selection method on AUC performance.},
author = {Zhou, Ligang and Lu, Dong and Fujita, Hamido},
doi = {10.1016/j.knosys.2015.04.017},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Zhou, Lu, Fujita - 2015 - The performance of corporate financial distress prediction models with features selection guided by domain kno.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Data mining,Domain knowledge,Features selection,Financial distress prediction,naranja,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,rojo},
month = {sep},
pages = {52--61},
title = {{The performance of corporate financial distress prediction models with features selection guided by domain knowledge and data mining approaches}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705115001616 http://linkinghub.elsevier.com/retrieve/pii/S0950705115001616},
volume = {85},
year = {2015}
}
@article{Bae2011,
abstract = {Many enterprises have been devoting a significant portion of their budget to product development in order to distinguish their products from those of their competitors and to make them better fit the needs and wants of customers. Hence, businesses should dBae, J. K., {\&} Kim, J. (2011). Product development with data mining techniques: A case on design of digital camera. Expert Systems with Applications, 38(8), 9274–9280. doi:10.1016/j.eswa.2011.01.030evelop product designing that could satisfy the customers' requirements since this will increase the enterprise's competitiveness and it is an essential criterion to earning higher loyalties and profits. This paper investigates the following research issues in the development of new digital camera products: (1) What exactly are the customers' "needs" and "wants" for digital camera products? (2) What features is more importance than others? (3) Can product design and planning for product lines/product collection be integrated with the knowledge of customers? (4) How can the rules help us to make a strategy during we design new digital camera? To investigate these research issues, the Apriori and C5.0 algorithms are methodologies of association rules and decision trees for data mining, which is implemented to mine customer's needs. Knowledge extracted from data mining results is illustrated as knowledge patterns and rules on a product map in order to propose possible suggestions and solutions for product design and marketing. ?? 2011 Published by Elsevier Ltd.},
author = {Bae, Jae Kwon and Kim, Jinhwa},
doi = {10.1016/j.eswa.2011.01.030},
file = {:Users/josepplloo/Documents/primer inmersion/experts systems.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Association rule,Data mining based methodology,Decision tree based models,New product development,rojo},
mendeley-groups = {estado del arte,con doi,propuesta,informe},
mendeley-tags = {rojo},
month = {aug},
number = {8},
pages = {9274--9280},
title = {{Product development with data mining techniques: A case on design of digital camera}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0957417411000509},
volume = {38},
year = {2011}
}
@book{Melorose2015,
address = {Berlin, Heidelberg},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Wohlin, Claes and Runeson, Per and H{\"{o}}st, Martin and Ohlsson, Magnus C. and Regnell, Bj{\"{o}}rn and Wessl{\'{e}}n, Anders},
booktitle = {Statewide Agricultural Land Use Baseline 2015},
doi = {10.1007/978-3-642-29044-2},
eprint = {arXiv:1011.1669v3},
file = {:Users/josepplloo/Documents/Material de base para todos los cap{\`{i}}tulos/Algunos libros y documentos externos de apoyo/Experimentation in Software Engineering.pdf:pdf},
isbn = {978-3-642-29043-5},
issn = {1098-6596},
keywords = {azul,icle},
mendeley-groups = {maerial base para el libro,con doi,informe},
mendeley-tags = {azul},
pmid = {25246403},
publisher = {Springer Berlin Heidelberg},
title = {{Experimentation in Software Engineering}},
url = {http://link.springer.com/10.1007/978-3-642-29044-2},
volume = {1},
year = {2012}
}
@inproceedings{Berger2013,
abstract = {Over more than two decades, numerous variability modeling techniques have been introduced in academia and industry. However, little is known about the actual use of these techniques. While dozens of experience reports on software product line engineering exist, only very few focus on variability modeling. This lack of empirical data threatens the validity of existing techniques, and hinders their improvement. As part of our effort to improve empirical understanding of variability modeling, we present the results of a survey questionnaire distributed to industrial practitioners. These results provide insights into application scenarios and perceived benefits of variability modeling, the notations and tools used, the scale of industrial models, and experienced challenges and mitigation strategies.},
address = {New York, New York, USA},
author = {Berger, Thorsten and Rublack, Ralf and Nair, Divya and Atlee, Joanne M. and Becker, Martin and Czarnecki, Krzysztof and W{\c{a}}sowski, Andrzej},
booktitle = {Proceedings of the Seventh International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '13},
doi = {10.1145/2430502.2430513},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/berger2013.pdf:pdf},
isbn = {9781450315418},
issn = {00985589},
keywords = {naranja,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,rojo},
pages = {1},
publisher = {ACM Press},
title = {{A survey of variability modeling in industrial practice}},
url = {http://dl.acm.org/citation.cfm?doid=2430502.2430513},
year = {2013}
}
@article{Dadaneh2016,
abstract = {Feature selection (FS) is one of the most important fields in pattern recognition, which aims to pick a subset of relevant and informative features from an original feature set. There are two kinds of FS algorithms depending on the presence of information about dataset class labels: supervised and unsupervised algorithms. Supervised approaches utilize class labels of dataset in the process of feature selection. On the other hand, unsupervised algorithms act in the absence of class labels, which makes their process more difficult. In this paper, we propose unsupervised probabilistic feature selection using ant colony optimization (UPFS). The algorithm looks for the optimal feature subset in an iterative process. In this algorithm, we utilize inter-feature information which shows the similarity between the features that leads the algorithm to decreased redundancy in the final set. In each step of the ACO algorithm, to select the next potential feature, we calculate the amount of redundancy between current feature and all those which have been selected thus far. In addition, we utilize a matrix to hold ant related pheromone which shows the rate of the co-presence of every pair of features in solutions. Afterwards, features are ranked based on a probability function extracted from the matrix; then, their m-top is returned as the final solution. We compare the performance of UPFS with 15 well-known supervised and unsupervised feature selection methods using different classifiers (support vector machine, naive Bayes, and k-nearest neighbor) on 10 well-known datasets. The experimental results show the efficiency of the proposed method compared to the previous related methods.},
author = {Dadaneh, Behrouz Zamani and Markid, Hossein Yeganeh and Zakerolhosseini, Ali},
doi = {10.1016/j.eswa.2016.01.021},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/1-s2.0-S0957417416000312-main.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Ant colony optimization,Classification accuracy,Feature selection,Filter approaches,Unsupervised methods,azul,naranja,purpura},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {azul,naranja,purpura},
month = {jul},
pages = {27--42},
publisher = {Elsevier Ltd},
title = {{Unsupervised probabilistic feature selection using ant colony optimization}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.01.021 http://linkinghub.elsevier.com/retrieve/pii/S0957417416000312},
volume = {53},
year = {2016}
}
@article{Zou2005a,
abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the pn case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00503.x},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/zou2005.pdf:pdf},
isbn = {1369-7412},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Grouping effect,LARS algorithm,Lasso,P ≫ n problem,Penalization,Variable selection,purpura,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura,rojo},
month = {apr},
number = {2},
pages = {301--320},
pmid = {20713001},
title = {{Regularization and variable selection via the elastic net}},
url = {http://doi.wiley.com/10.1111/j.1467-9868.2005.00503.x},
volume = {67},
year = {2005}
}
@inproceedings{Zhang2013,
abstract = {As a Software Product Line (SPL) evolves variability specifications in problem space and variability realizations in solution space erode over time and impact productivity during development. On the one hand, the variability model tends to be incomplete and inconsistent with the core assets; on the other hand, the core assets become overly complex, which make them difficult to understand and maintain. In this paper, we present the RECoVar framework towards reverse engineering of SPL variability. The framework includes two approaches: a) code-based variability model extraction; and b) complex feature correlation mining. These two approaches help to extract various variability information, so that variability specifications and realizations can be maintained in an efficient way.},
author = {Zhang, Bo and Becker, Martin},
booktitle = {2013 4th International Workshop on Product LinE Approaches in Software Engineering (PLEASE)},
doi = {10.1109/PLEASE.2013.6608664},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/06608664.pdf:pdf},
isbn = {978-1-4673-6449-2},
keywords = {Variability modeling,naranja,product line analysis,reverse engineering},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
month = {may},
pages = {45--48},
publisher = {IEEE},
title = {{RECoVar: A solution framework towards reverse engineering variability}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6608664},
year = {2013}
}
@inproceedings{Lian2015,
abstract = {As an important research issue in software product line, feature selection is extensively studied. Besides the basic functional requirements (FRs), the non-functional requirements (NFRs) are also critical during feature selection. Some NFRs have numerical constraints, while some have not. Without clear criteria, the latter are always expected to be the best possible. However, most existing selection methods ignore the combination of constrained and unconstrained NFRs and FRs. Meanwhile, the complex constraints and dependencies among features are perpetual challenges for feature selection. To this end, this paper proposes a multi-objective optimization algorithm IVEA to optimize the selection of features with NFRs and FRs by considering the relations among these features. Particularly, we first propose a two-dimensional fitness function. One dimension is to optimize the NFRs without quantitative constraints. The other one is to assure the selected features satisfy the FRs, and conform to the relations among features. Second, we propose a violation-dominance principle, which guides the optimization under FRs and the relations among features. We conducted comprehensive experiments on two feature models with different sizes to evaluate IVEA with state-of-the-art multi-objective optimization algorithms, including IBEAHD, IBEA$\epsilon$+, NSGA-II and SPEA2. The results showed that the IVEA significantly outperforms the above baselines in the NFRs optimization. Meanwhile, our algorithm needs less time to generate a solution that meets the FRs and the constraints on NFRs and fully conforms to the feature model.},
author = {Lian, Xiaoli and Zhang, Li},
booktitle = {2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},
doi = {10.1109/SANER.2015.7081829},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/lian2015.pdf:pdf},
isbn = {978-1-4799-8469-5},
keywords = {Evolutionary computation,Feature Models,Feature Selection,IBEAHD,IBEA$\epsilon$+,IVEA,Multi-objective Optimization,NFR optimization,NSGA-II,Non-functional requirements optimization,Optimization,Portals,SPEA2,Security,Sociology,Software,Software Product Line,Statistics,feature selection,genetic algorithms,multiobjective optimization algorithm,nonfunctional requirements,numerical constraint,optimized feature selection,purpura,rojo,selection method,software product line,software product lines,two-dimensional fitness function,violation-dominance principle},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura,rojo},
month = {mar},
pages = {191--200},
publisher = {IEEE},
title = {{Optimized feature selection towards functional and non-functional requirements in Software Product Lines}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7081829},
year = {2015}
}
@inproceedings{Capilla2014,
address = {New York, New York, USA},
author = {Capilla, Rafael},
booktitle = {Proceedings of the 18th International Software Product Line Conference on Companion Volume for Workshops, Demonstrations and Tools - SPLC '14},
doi = {10.1145/2647908.2655960},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/capilla2014.pdf:pdf},
isbn = {9781450327398},
keywords = {azul},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {azul},
pages = {13--13},
publisher = {ACM Press},
title = {{From feature modeling to context variability modeling}},
url = {http://dl.acm.org/citation.cfm?doid=2647908.2655960},
year = {2014}
}
@article{Hariri2013,
abstract = {Domain analysis is a labor-intensive task in which related software systems are analyzed to discover their common and variable parts. Many software projects include extensive domain analysis activities, intended to jumpstart the requirements process through identifying potential features. In this paper, we present a recommender system that is designed to reduce the human effort of performing domain analysis. Our approach relies on data mining techniques to discover common features across products as well as relationships among those features. We use a novel incremental diffusive algorithm to extract features from online product descriptions, and then employ association rule mining and the (k)-nearest neighbor machine learning method to make feature recommendations during the domain analysis process. Our feature mining and feature recommendation algorithms are quantitatively evaluated and the results are presented. Also, the performance of the recommender system is illustrated and evaluated within the context of a case study for an enterprise-level collaborative software suite. The results clearly highlight the benefits and limitations of our approach, as well as the necessary preconditions for its success.},
author = {Hariri, Negar and Castro-Herrera, Carlos and Mirakhorli, Mehdi and Cleland-Huang, Jane and Mobasher, Bamshad},
doi = {10.1109/TSE.2013.39},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/06582404.pdf:pdf},
isbn = {0098-5589 VO - 39},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Domain analysis,association rule mining,clustering,k-nearest neighbor,naranja,purpura,recommender systems,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,purpura,rojo},
month = {dec},
number = {12},
pages = {1736--1752},
title = {{Supporting Domain Analysis through Mining and Recommending Features from Online Product Listings}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6582404},
volume = {39},
year = {2013}
}
@article{Ayaz2015,
abstract = {Compressive strength and UPV parameters are the methods that are used to determine high-volume mineral admixture concrete quality. But experiments for all levels of these parameters are expensive, difficult and time consuming. For determination of output values, classifiers with model extraction features can be used. In this study, classifiers, with the rule-based M5 rule and tree model M5P in the area of data mining are used to predict the compressive strength and UPV of concrete mixtures after 3, 7, 28 and 120 days of curing. The M5 rule and tree model M5P are tested using the available test data of 40 different concrete mix-designs gathered from literature [1]. The input of the model is a variable data set corresponding to concrete mixture proportions. The findings of this study indicated that the M5 rule and tree model M5P models are sufficient tools for estimating the compressive strength and UPV of concrete. 97{\{}{\%}{\}} and 87{\{}{\%}{\}} success is obtained in predicting compressive strength and UPV results, respectively.},
author = {Ayaz, Yaşar and Kocamaz, Adnan Fatih and Karako{\c{c}}, Mehmet Burhan},
doi = {10.1016/j.conbuildmat.2015.06.029},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Ayaz, Kocamaz, Karako{\c{c}} - 2015 - Modeling of compressive strength and UPV of high-volume mineral-admixtured concrete using rule-based M5.pdf:pdf},
issn = {09500618},
journal = {Construction and Building Materials},
keywords = {Compressive strength,Concrete,Data mining,M5 rule,Tree model M5P,UPV,purpura},
mendeley-groups = {con doi,informe},
mendeley-tags = {purpura},
month = {sep},
pages = {235--240},
title = {{Modeling of compressive strength and UPV of high-volume mineral-admixtured concrete using rule-based M5 rule and tree model M5P classifiers}},
url = {http://www.sciencedirect.com/science/article/pii/S0950061815007114 http://linkinghub.elsevier.com/retrieve/pii/S0950061815007114},
volume = {94},
year = {2015}
}
@inproceedings{Farias2016,
address = {New York, New York, USA},
author = {{de F. Farias}, M{\'{a}}rio Andr{\'{e}} and Novais, Renato and J{\'{u}}nior, Methanias Cola{\c{c}}o and {da Silva Carvalho}, Lu{\'{i}}s Paulo and Mendon{\c{c}}a, Manoel and Sp{\'{i}}nola, Rodrigo Oliveira},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing - SAC '16},
doi = {10.1145/2851613.2851786},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/sms1.pdf:pdf},
isbn = {9781450337397},
keywords = {Systematic mapping study,empirical,empirical software engineering,gris,mining software repository,secondary study,software engineering,systematic mapping study},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris},
pages = {1472--1479},
publisher = {ACM Press},
title = {{A systematic mapping study on mining software repositories}},
url = {http://dl.acm.org/citation.cfm?doid=2851613.2851786},
year = {2016}
}
@article{Elovici2003,
author = {Elovici, Yuval and Braha, Dan},
doi = {10.1109/TSMCA.2003.812596},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Elovici, Braha - 2003 - A decision-theoretic approach to data mining.pdf:pdf},
issn = {1083-4427},
journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
mendeley-groups = {estado del arte,propuesta,informe},
month = {jan},
number = {1},
pages = {42--51},
title = {{A decision-theoretic approach to data mining}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1206454 http://ieeexplore.ieee.org/document/1206454/},
volume = {33},
year = {2003}
}
@article{Davril2015,
abstract = {Feature models have become one of the most widely used formalism for representing the variability among the products of a product line. The design of a feature model from a set of existing products can help stakeholders communicate on the commonalities and differences between the products, facilitate the adoption of mass customization strategies, or support the definition of the solution space of a product configurator (i.e. The sets of products that will be and will not be offered to the targeted customers). As the manual construction of feature models proves to be a time-consuming and error prone task, researchers have proposed various approaches for automatically deriving feature models from available product data. Existing reverse engineering techniques mostly rely on data mining algorithms that search for frequently occurring patterns between the features of the available product configurations. However, when the number of features is too large, the sparsity among the configurations can reduce the quality of the extracted model. In this paper, we discuss motivations for the development of dimensionality reduction techniques for product lines in order to support the extraction of feature models in the case of high-dimensional product spaces. We use a real world dataset to illustrate the problems arising with high dimensionality and present four research questions to address them.},
author = {Davril, Jean Marc and Heymans, Patrick and B{\'{e}}can, Guillaume and Acher, Mathieu},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/04{\_}DavrilHeymansBecanAcher{\_}OnBreakingTheCurseOfDimensionality{\_}Confws-15{\_}p19.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
pages = {19--22},
title = {{On breaking the curse of dimensionality in reverse engineering feature models}},
url = {http://ceur-ws.org/Vol-1453/04{\_}DavrilHeymansBecanAcher{\_}OnBreakingTheCurseOfDimensionality{\_}Confws-15{\_}p19.pdf},
volume = {1453},
year = {2015}
}
@article{B??can2015,
abstract = {Feature Models (FMs) are a popular formalism for modeling and reasoning about the configurations of a software product line. As the manual construction of an FM is time-consuming and error-prone, management operations have been developed for reverse engineering, merging, slicing, or refactoring FMs from a set of configurations/dependencies. Yet the synthesis of meaningless ontological relations in the FM – as defined by its feature hierarchy and feature groups – may arise and cause severe difficulties when reading, maintaining or exploiting it. Numerous synthesis techniques and tools have been proposed, but only a few consider both configuration and ontolog-ical semantics of an FM. There are also few empirical studies investigating ontological aspects when synthesizing FMs. In this article, we define a generic, ontologic-aware synthesis procedure that computes the likely siblings or parent candidates for a given feature. We develop six heuristics for clustering and weighting the logical, syntactical and semantical relationships between feature names. We then perform an empirical evaluation on hundreds of FMs, coming from the SPLOT repository and Wikipedia. We provide evidence that a fully automated synthesis (i.e., without any user intervention) is likely to produce FMs far from the ground truths. As the role of the user is crucial, we empirically analyze the strengths and weak-nesses of heuristics for computing ranking lists and different kinds of clusters. We show that a hybrid approach mixing logical and ontological techniques outperforms state-of-the-art solutions. We believe our approach, environment, and empirical results support researchers and practitioners working on reverse engineering and management of FMs.},
author = {B{\'{e}}can, Guillaume and Acher, Mathieu and Baudry, Benoit and Nasr, Sana Ben},
doi = {10.1007/s10664-014-9357-1},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/springer/10.1007@s10664-014-9357-1.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
keywords = {Feature model,Model management,Refactoring,Reverse engineering,Software product lines,Variability,gris},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris},
month = {aug},
number = {4},
pages = {1794--1841},
title = {{Breathing ontological knowledge into feature model synthesis: an empirical study}},
url = {http://link.springer.com/10.1007/s10664-014-9357-1},
volume = {21},
year = {2016}
}
@article{Miotto2016,
author = {Miotto, Riccardo and Li, Li and Kidd, Brian A. and Dudley, Joel T.},
doi = {10.1038/srep26094},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/nature/srep26094.pdf:pdf},
issn = {2045-2322},
journal = {Scientific Reports},
keywords = {rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
month = {may},
number = {April},
pages = {26094},
pmid = {27185194},
title = {{Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records}},
url = {http://www.nature.com/articles/srep26094},
volume = {6},
year = {2016}
}
@article{Thum2014,
abstract = {Software-product-line engineering has gained considerable momentum in recent years, both in industry and in academia. A software product line is a family of software products that share a common set of features. Software product lines challenge traditional analysis techniques, such as type checking, model checking, and theorem proving, in their quest of ensuring correctness and reliability of software. Simply creating and analyzing all products of a product line is usually not feasible, due to the potentially exponential number of valid feature combinations. Recently, researchers began to develop analysis techniques that take the distinguishing properties of software product lines into account, for example, by checking feature-related code in isolation or by exploiting variability information during analysis. The emerging field of product-line analyses is both broad and diverse, so it is difficult for researchers and practitioners to understand their similarities and differences. We propose a classification of product-line analyses to enable systematic research and application. Based on our insights with classifying and comparing a corpus of 123 research articles, we develop a research agenda to guide future research on product-line analyses.},
author = {Th{\"{u}}m, Thomas and Apel, Sven and K{\"{a}}stner, Christian and Schaefer, Ina and Saake, Gunter},
doi = {10.1145/2580950},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/10.1145@2580950.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
keywords = {verde},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {verde},
month = {jun},
number = {1},
pages = {1--45},
title = {{A Classification and Survey of Analysis Strategies for Software Product Lines}},
url = {http://dl.acm.org/citation.cfm?doid=2620784.2580950},
volume = {47},
year = {2014}
}
@article{Kastner2014,
abstract = {Software product line engineering is an efficient means to generate a set of tailored software products from a common implementation. However, adopting a product-line approach poses a major challenge and significant risks, since typically legacy code must be migrated toward a product line. Our aim is to lower the adoption barrier by providing semi-automatic tool support-called variability mining -to support developers in locating, documenting, and extracting implementations of product-line features from legacy code. Variability mining combines prior work on concern location, reverse engineering, and variability-aware type systems, but is tailored specifically for the use in product lines. Our work pursues three technical goals: (1) we provide a consistency indicator based on a variability-aware type system, (2) we mine features at a fine level of granularity, and (3) we exploit domain knowledge about the relationship between features when available. With a quantitative study, we demonstrate that variability mining can efficiently support developers in locating features.},
author = {Kastner, Christian and Dreiling, Alexander and Ostermann, Klaus},
doi = {10.1109/TSE.2013.45},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/kastner2013.pdf:pdf},
isbn = {0098-5589 VO - 40},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {LEADT,Variability,feature,feature location,gris,mining,naranja,reverse engineering,software product line},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris,naranja},
month = {jan},
number = {1},
pages = {67--82},
title = {{Variability Mining: Consistent Semi-automatic Detection of Product-Line Features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6613490},
volume = {40},
year = {2014}
}
@inproceedings{London2015,
abstract = {In this study, we discuss the possible application of the ubiquitous complex network approach for information extraction from educational data. Since a huge amount of data (which is detailed as well) is produced by the complex administration systems of educational institutes, instead of the classical statistical methods, new types of data processing techniques are required to handle it. We define several suitable network representations of students, teachers and subjects in public education and present some possible ways of how graph mining techniques can be used to get detailed information about them. Depending on the construction of the underlying graph, we examine several network models and discuss which are the most appropriate graph mining tools (like community detection and ranking and centrality measures) that can be applied on them. Lastly, we attempt to highlight the many advantages of using graph-based data mining in educational data against the classical evaluation techniques.},
address = {New York, New York, USA},
author = {London, Andr{\'{a}}s and Pelyhe, {\'{A}}ron and Holl{\'{o}}, Csaba and N{\'{e}}meth, Tam{\'{a}}s},
booktitle = {Proceedings of the 16th International Conference on Computer Systems and Technologies - CompSysTech '15},
doi = {10.1145/2812428.2812436},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/london2015.pdf:pdf},
isbn = {9781450333573},
keywords = {complex networks,data mining,educational evaluation,graph mining,mathematical modeling,purpura},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura},
pages = {358--365},
publisher = {ACM Press},
title = {{Applying graph-based data mining concepts to the educational sphere}},
url = {http://doi.acm.org/10.1145/2812428.2812436 http://dl.acm.org/citation.cfm?doid=2812428.2812436},
year = {2015}
}
@article{Darabad2015,
abstract = {Suggestion and application of a set of new features for on-line Partial Discharge (PD) monitoring, where there is no information about the type of PD is a challenging task for condition assessment of power equipments, such as a power transformer. This is looked for in this paper. So far, in past various techniques have been employed to develop a comprehensive PD monitoring system, however limited success has been achieved. One of the challenging issues in this field is the discovering of proper features capable of differentiating the involvement of possible types of PD sources. In order to examine the efficiency of the method established in this paper, which is based on application of a set of new feature spaces, texture feature analysis, followed by application of principal component analysis (PCA) and self-organizing map (SOM) is used to analyze and interpret the time-domain-captured PD data. The results of this work demonstrate the capabilities of the aforementioned features space to be used as a supplementary knowledge-base to help experts making their decisions confidently.},
author = {Darabad, V.P. and Vakilian, M. and Blackburn, T.R. and Phung, B.T.},
doi = {10.1016/j.ijepes.2015.03.016},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Darabad et al. - 2015 - An efficient PD data mining method for power transformer defect models using SOM technique.pdf:pdf},
issn = {01420615},
journal = {International Journal of Electrical Power {\&} Energy Systems},
keywords = {BMU,Ch,DDF,DGA,DWT,Data mining,Defect model,FRA,GLCM,GLDV,Gs/s,HFCT,HV,IR,Lab,Mpts,Ms/s,Ng,PC,PCA,PD,PDC,PRPD,Partial discharge,Power transformer,RVM,SNE,SOM,SVM,Support Vector Machine,best match unit,channel,dielectric dissipation factor,discrete wavelet transform,dissolved gas analysis,frequency response analysis,giga sample per second,gray level covariance matrix,gray level difference vector,gray level value,high frequency current transformer,high voltage,insulation resistance,laboratory,mega points,mega sample per second,naranja,p.f.,partial discharge,phase resolved partial discharge,polarization and depolarization current,power frequency,principal component,principal component analysis,recovery voltage measurement,self-organizing map,stochastic neighbor embedding},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
month = {oct},
pages = {373--382},
title = {{An efficient PD data mining method for power transformer defect models using SOM technique}},
url = {http://www.sciencedirect.com/science/article/pii/S0142061515001519 http://linkinghub.elsevier.com/retrieve/pii/S0142061515001519},
volume = {71},
year = {2015}
}
@article{Farid2014,
abstract = {In this paper, we introduce two independent hybrid mining algorithms to improve the classification accuracy rates of decision tree (DT) and na{\"{i}}ve Bayes (NB) classifiers for the classification of multi-class problems. Both DT and NB classifiers are useful, efficient and commonly used for solving classification problems in data mining. Since the presence of noisy contradictory instances in the training set may cause the generated decision tree suffers from overfitting and its accuracy may decrease, in our first proposed hybrid DT algorithm, we employ a na{\"{i}}ve Bayes (NB) classifier to remove the noisy troublesome instances from the training set before the DT induction. Moreover, it is extremely computationally expensive for a NB classifier to compute class conditional independence for a dataset with high dimensional attributes. Thus, in the second proposed hybrid NB classifier, we employ a DT induction to select a comparatively more important subset of attributes for the production of na{\"{i}}ve assumption of class conditional independence. We tested the performances of the two proposed hybrid algorithms against those of the existing DT and NB classifiers respectively using the classification accuracy, precision, sensitivity-specificity analysis, and 10-fold cross validation on 10 real benchmark datasets from UCI (University of California, Irvine) machine learning repository. The experimental results indicate that the proposed methods have produced impressive results in the classification of real life challenging multi-class problems. They are also able to automatically extract the most valuable training datasets and identify the most effective attributes for the description of instances from noisy complex training databases with large dimensions of attributes. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Farid, Dewan Md and Zhang, Li and Rahman, Chowdhury Mofizur and Hossain, M.A. and Strachan, Rebecca},
doi = {10.1016/j.eswa.2013.08.089},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/1-s2.0-S0957417413007100-main.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classification,Data mining,Decision tree,Hybrid,Na{\"{i}}ve Bayes classifier,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
month = {mar},
number = {4},
pages = {1937--1946},
publisher = {Elsevier Ltd},
title = {{Hybrid decision tree and na{\"{i}}ve Bayes classifiers for multi-class classification tasks}},
url = {http://dx.doi.org/10.1016/j.eswa.2013.08.089 http://linkinghub.elsevier.com/retrieve/pii/S0957417413007100},
volume = {41},
year = {2014}
}
@article{Souag2015,
author = {Souag, Amina and Mazo, Ra{\'{u}}l and Salinesi, Camille and Comyn-Wattiau, Isabelle},
doi = {10.1007/s00766-015-0220-8},
file = {:Users/josepplloo/Documents/primer inmersion/Reusable knowledge in SRE V15.pdf:pdf},
issn = {0947-3602},
journal = {Requirements Engineering},
keywords = {verde},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {verde},
month = {jun},
number = {2},
pages = {251--283},
title = {{Reusable knowledge in security requirements engineering: a systematic mapping study}},
url = {http://link.springer.com/10.1007/s00766-015-0220-8},
volume = {21},
year = {2016}
}
@article{Lin2013,
abstract = {Multiple-generation product lines require carefully planned strategies in order to reap the benefits of utilizing technology assets and resources efficiently over an elongated time span. In this paper, we build on our previous work in which we successfully implemented dynamic variable state models (DVSMs) to forecast the introduction timing of future generations for an existing multiple-generation product line. Here we implement DVSMs for the design of a new multiple-generation product line. We investigate the potential for using historical sales data about similar products to generate a complete set of forecasts and relevant strategic moves using DVSMs. We present a case study implementing the proposed framework on Apple Inc.'s iPad product line. Results show that the forecast performance of the model matches the realized real data, and hence we deem the DSVM to be appropriate for modeling a new multi-generation product line. {\textcopyright} 2012 Elsevier Ltd. All rights reserved.},
author = {Lin, Chun-Yu and Okudan, G{\"{u}}l E.},
doi = {10.1016/j.eswa.2012.10.022},
file = {:Users/josepplloo/Downloads/Planning for multiple-generation product lines.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Dynamic state variable models,Multiple generation product lines,purpura},
mendeley-groups = {estado del arte,con doi,propuesta,informe},
mendeley-tags = {purpura},
month = {may},
number = {6},
pages = {2013--2022},
publisher = {Elsevier Ltd},
title = {{Planning for multiple-generation product lines using dynamic variable state models with data input from similar products}},
url = {http://dx.doi.org/10.1016/j.eswa.2012.10.022 http://linkinghub.elsevier.com/retrieve/pii/S0957417412011360},
volume = {40},
year = {2013}
}
@inproceedings{Zaid2009,
abstract = {Feature models are models used to capture differences and commonalities between software features, enabling the representation of variability within software. There are many variations of feature models and different notations are often used to represent the same information. Currently support for validating or integrating feature models is missing. In this paper, we provide an ontology framework for feature modeling which consists of an ontology that formally provides a specification for feature models. In addition, we provide means to integrate segmented feature models and provide a rule based model consistency check and conflict detection. We use SWRL rules to implement the rules and a DL reasoner to evaluate the rules and infer extra interesting information regarding the variability of the software.},
address = {New York, New York, USA},
author = {Zaid, Lamia Abo and Kleinermann, Frederic and {De Troyer}, Olga},
booktitle = {Proceedings of the 2009 ACM symposium on Applied Computing - SAC '09},
doi = {10.1145/1529282.1529563},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/zaid2009.pdf:pdf},
isbn = {9781605581668},
keywords = {Feature models,OWL,naranja,ontologies,software variabil},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
pages = {1252},
publisher = {ACM Press},
title = {{Applying semantic web technology to feature modeling}},
url = {http://portal.acm.org/citation.cfm?doid=1529282.1529563},
year = {2009}
}
@article{Tuarob2015,
author = {Tuarob, Suppawong and Tucker, Conrad S.},
doi = {10.1115/1.4030049},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/asme/jcise{\_}015{\_}03{\_}031003.pdf:pdf},
issn = {1050-0472},
journal = {Journal of Mechanical Design},
keywords = {rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
month = {jul},
number = {7},
pages = {071402},
title = {{Automated Discovery of Lead Users and Latent Product Features by Mining Large Scale Social Media Networks}},
url = {http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?doi=10.1115/1.4030049},
volume = {137},
year = {2015}
}
@article{Botia2013,
abstract = {Fuzzy clustering allows finding classes through the historical data in order to associate them with functional states useful to represent the complex industrial processes behavior. By means of classes, an automaton can be established that determines the current and the next connections of functional states of a process. When fuzzy clustering is used, the connections in the historical data are considered but it does not find other important connections. To solve this limitation, a new method to seek the most important connections among functional states is proposed. Initially, the approach defines an initial transition degrees matrix, where all connections are taken into account. Through a proposed update step, the most important connections are obtained, which they describe the real behavior of a process. In addition, a new distance criterion is defined to improve the update step. The final transition degrees matrix is used to construct a fuzzy automaton that it is validated by human operator's experience. The approach was tested in a steam generator process. Applying three fuzzy clustering algorithms in case of study, the proposed method finds the same transition matrix. The new connections were validated by the human operator. Crown Copyright {\textcopyright} 2012 Published by Elsevier Ltd. All rights reserved.},
author = {Bot{\'{i}}a, Javier F. and Isaza, Claudia and Kempowsky, Tatiana and {Le Lann}, Marie V{\'{e}}ronique and Aguilar-Mart{\'{i}}n, Joseph},
doi = {10.1016/j.engappai.2012.11.003},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/Automaton Based on Fuzzy Clustering Methods for Monitoring Industrial Processes.pdf:pdf},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Fuzzy automaton,Fuzzy clustering method,Hebbian functions},
mendeley-groups = {estado del arte,con doi,informe},
month = {apr},
number = {4},
pages = {1211--1220},
title = {{Automaton based on fuzzy clustering methods for monitoring industrial processes}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0952197612002886},
volume = {26},
year = {2013}
}
@article{Nadi2015,
author = {Nadi, Sarah and Kr{\"{u}}ger, Stefan},
doi = {10.1145/2866614.2866629},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/nadi2016.pdf:pdf},
isbn = {978-1-4503-4019-9},
journal = {Proceedings of the Tenth International Workshop on Variability Modelling of Software-intensive Systems},
keywords = {Clafer,Cryptography,Variability Modeling,purpura,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura,rojo},
pages = {105--112},
title = {{Variability Modeling of Cryptographic Components: Clafer Experience Report}},
url = {http://doi.acm.org/10.1145/2866614.2866629},
year = {2015}
}
@article{Muller2011,
abstract = {Today many software systems are developed for a wide even anonymous audience. If needs of customers are too diverse, offering one software product can be too inflexible. In that situation offering a Software Product Line (SPL) can be economically advantageous. However, product management of SPLs has to decide what features to realize. This decision requires information on the utility features spend to customers and customer's willingness to pay (WTP). Conjoint Analysis (CA) is a promising way to measure both. However, the design and implementation of a conjoint study is costly even though much of the work to design a conjoint study has already been done in Software Product Line Engineering (SPLE). To utilize the work of SPLE for the design of conjoint surveys, we propose a procedure to automatically derive conjoint surveys from Feature Models and report on our implementation of a conjoint survey generator and a web-based surveying tool.},
author = {Muller, Johannes and Lillack, Max},
doi = {10.1109/SEAA.2011.73},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/muller2011.pdf:pdf},
isbn = {978-1-4577-1027-8},
journal = {2011 37th EUROMICRO Conference on Software Engineering and Advanced Applications},
keywords = {-software product line,conjoint analysis,feature modeling,gris,marketing,ment,product manage-,scoping},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {gris},
pages = {374--377},
title = {{Conjoint Analysis of Software Product Lines: A Feature Based Approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6068370},
year = {2011}
}
@book{Kwak2011,
address = {New York, NY},
author = {Kwak, Minjung and Kim, Harrison},
booktitle = {Advances in Product Family and Product Platform Design},
doi = {10.1007/978-1-4614-7937-6},
editor = {Simpson, Timothy W. and Jiao, Jianxin and Siddique, Zahed and H{\"{o}}ltt{\"{a}}-Otto, Katja},
file = {:Users/josepplloo/Documents/Material de base para todos los cap{\`{i}}tulos/Algunos libros y documentos externos de apoyo/Advances in Product Family and Product Platform Design.pdf:pdf},
isbn = {978-1-4614-7936-9},
keywords = {naranja,rojo},
mendeley-groups = {maerial base para el libro,con doi,informe},
mendeley-tags = {naranja,rojo},
number = {3},
pages = {707--735},
publisher = {Springer New York},
title = {{Advances in Product Family and Product Platform Design}},
url = {http://link.springer.com/10.1007/978-1-4614-7937-6},
volume = {43},
year = {2014}
}
@article{Ndjodo2010,
author = {Ndjodo, Marcel Fouda and Ngoumou, Amougou},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/doaj/7-5-382-393.pdf:pdf},
keywords = {feature-,product line engineering,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
number = {5},
title = {{Product Lines ' Feature-Oriented Engineering for Reuse : A Formal Approach}},
volume = {7},
year = {2010}
}
@article{Thurimella2012,
abstract = {Context: Variability management is a key activity in software product line engineering. This paper focuses on managing rationale information during the decision-making activities that arise during variability management. By decision-making we refer to systematic problem solving by considering and evaluating various alternatives. Rationale management is a branch of science that enables decision-making based on the argumentation of stakeholders while capturing the reasons and justifications behind these decisions. Objective: Decision-making should be supported to identify variability in domain engineering and to resolve variation points in application engineering. We capture the rationale behind variability management decisions. The captured rationale information is useful to evaluate future changes of variability models as well as to handle future instantiations of variation points. We claim that maintaining rationale will enhance the longevity of variability models. Furthermore, decisions should be performed using a formal communication between domain engineering and application engineering. Method: We initiate the novel area of issue-based variability management (IVM) by extending variability management with rationale management. The key contributions of this paper are: (i) an issue-based variability management methodology (IVMM), which combines questions, options and criteria (QOC) and a specific variability approach; (ii) a meta-model for IVMM and a process for variability management and (iii) a tool for the methodology, which was developed by extending an open source rationale management tool. Results: Rationale approaches (e.g. questions, options and criteria) guide distributed stakeholders when selecting choices for instantiating variation points. Similarly, rationale approaches also aid the elicitation of variability and the evaluation of changes. The rationale captured within the decision-making process can be reused to perform future decisions on variability. Conclusion: IVMM was evaluated comparatively based on an experimental survey, which provided evidence that IVMM is more effective than a variability modeling approach that does not use issues. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Thurimella, Anil Kumar and Bruegge, Bernd},
doi = {10.1016/j.infsof.2012.02.005},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/10.1016@j.infsof.2012.02.005.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Empirical software engineering,Product line engineering,Rationale management,Requirements engineering,naranja,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,rojo},
month = {sep},
number = {9},
pages = {933--950},
publisher = {Elsevier B.V.},
title = {{Issue-based variability management}},
url = {http://dx.doi.org/10.1016/j.infsof.2012.02.005 http://linkinghub.elsevier.com/retrieve/pii/S0950584912000481},
volume = {54},
year = {2012}
}
@incollection{Acher2010,
abstract = {The use of Feature Models (FMs) to define the valid combinations of features in Software Product Lines (SPL) is becoming commonplace. To enhance the scalability of FMs, support for composing FMs describing different SPL aspects is needed. Some composition operators, with interesting property preservation capabilities, have already been defined but a comprehensive and efficient implementation is still to be proposed. In this paper, we systematically compare strengths and weaknesses of different implementation approaches. The study provides some evidence that using generic model composition frameworks are not helping much in the realization, whereas a specific solution is finally necessary and clearly stands out by its qualities.},
annote = {Un diagrama de caracteristicas es la representaci{\'{o}}n de una familia.
Un FM describe el conjunto de las combinaciones validas entre caracteristicas.
En la actualidad la creaci{\'{o}}n de los modelos de caracteristicas depende de los procesos industriales(servicios),cada uno tiene su modelo de caracteristicas y el modelo de caracteristicas general se puede sacar con la intersecci{\'{o}}n de los modelos en todo el proceso.
se conocen como Multi level features
trees
AGG (sistema de trasnformaci{\'{o}}n algebraico)Taentzer, G es una interfaz gr{\'{a}}fica que pretende pasar del algebra relacional a los grafos y generar nuevos grafos a partir de unos ya existentes.
En este articulo se presenta una comparativa de estos software 
EL sosftware AGG, Kompose provee una resistencia a fallar en las operaciones donde no se puede generar un FM por que no hay suficientes caracteristicas.
MBE y AOM no tienen destreza para implementar la union de modelos de caracteristicas.
Las trasnformaciones semanticas o los modelos de preservaci{\'{o}}n semantica guardan o incluyen la semantica a diferencia de los anteriores},
author = {Acher, Mathieu and Collet, Philippe and Lahire, Philippe and France, Robert},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-13595-8_3},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/springer/10.1007@978-3-642-13595-83.pdf:pdf},
isbn = {3642135943},
issn = {03029743},
keywords = {naranja,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,rojo},
pages = {3--19},
title = {{Comparing Approaches to Implement Feature Model Composition}},
url = {http://link.springer.com/10.1007/978-3-642-13595-8{\_}3},
volume = {6138 LNCS},
year = {2010}
}
@article{White2008,
abstract = {Product-line architectures (PLAs) are an effective mechanism for facilitating the reuse of software components on different mobile devices. Mobile applications are typically delivered to devices using over-the-air provisioning services that allow a mobile phone to download and install software over a cellular network connection. Current techniques for automating product-line variant selection do not address the unique requirements (such as the need to consider resource constraints) of dynamically selecting a variant for over- the-air provisioning. This paper presents the following contributions to product-line variant selection for mobile devices: (1) it describes how a constraint solver can be used to dynamically select a product-line variant while adhering to resource constraints, (2) it presents architectures for automatically discovering device capabilities mapping them to product-line feature models, (3) and it includes results from experiments and field tests with an automated variant selector, and (4) it describes PLA design rules that can be used to increase the performance of automated constraint-based variant selection. Our empirical results show that fast au- tomated variant selection from a feature model is possible if certain product-line design guidelines are followed.},
author = {White, Jules and Schmidt, DouglasC. C and Wuchner, Egon and Nechypurenko, Andrey},
doi = {10.1007/BF03192550},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/SciELO/a04v14n1.pdf:pdf},
isbn = {01046500 (ISSN)},
issn = {0104-6500},
journal = {Journal of the Brazilian Computer Society},
keywords = {Constraint Satisfaction,constraint,feature modeling,product-lines,rojo,satisfaction,software reuse},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
month = {mar},
number = {1},
pages = {25--44},
title = {{Automatically composing reusable software components for mobile devices}},
url = {http://dx.doi.org/10.1007/BF03192550 http://link.springer.com/10.1007/BF03192550},
volume = {14},
year = {2008}
}
@article{Bhuyan2015,
abstract = {This paper addresses the selection of sub-feature from each feature using fuzzy methodologies maintaining the privacy during collection of data from participating parties in distributed environment. Based on fuzzy random variables conditional expectation is used in which two fuzzy sets are generated using Borel set that helps to determine sub-feature within certain interval. The privacy and selection of sub-feature leading to a distinguished class is the main objective of this research work. These two problems are directly related to data mining problems of classification and characterization of feature. In many cases traditional techniques are not suitable for complex databases. However our methodology provides better way for selection of sub-features under different situations. The proposed model and techniques both presents extensive theoretical analysis and experimental results. The experiments show the effectiveness and performance based on real world data set.},
author = {Bhuyan, Hemanta Kumar and Kamila, Narendra Kumar},
doi = {10.1016/j.asoc.2015.06.060},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Bhuyan, Kamila - 2015 - Privacy preserving sub-feature selection in distributed data mining.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing},
keywords = {Distributed data mining,Feature selection,Fuzzy probability,Fuzzy random variable,Privacy,purpura},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura},
month = {nov},
pages = {552--569},
title = {{Privacy preserving sub-feature selection in distributed data mining}},
url = {http://www.sciencedirect.com/science/article/pii/S1568494615004536 http://linkinghub.elsevier.com/retrieve/pii/S1568494615004536},
volume = {36},
year = {2015}
}
@inproceedings{Rothberg2016,
address = {New York, New York, USA},
author = {Rothberg, Valentin and Dintzner, Nicolas and Ziegler, Andreas and Lohmann, Daniel},
booktitle = {Proceedings of the Tenth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '16},
doi = {10.1145/2866614.2866624},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/rothberg2016.pdf:pdf},
isbn = {9781450340199},
keywords = {CADOS,Configurability,Feature Models,Kconfig,Linux,naranja,rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,rojo},
pages = {65--72},
publisher = {ACM Press},
title = {{Feature Models in Linux}},
url = {http://doi.acm.org/10.1145/2866614.2866624 http://dl.acm.org/citation.cfm?doid=2866614.2866624},
year = {2016}
}
@article{Felfernig2015a,
abstract = {Knowledge about formal (semantic) interpretations of natural language domain descriptions is crucial for avoiding communication overheads between domain experts and knowledge engineers. In this paper we report preliminary results of an empirical study in which we investigated in which way natural language statements are formalized by knowledge engineers. We summarize the findings of our study and discuss aspects to be taken into account in order to avoid additional (often not needed) iterations in configuration knowledge engineering processes. This work is exploratory and intended to figure out open issues for future work.},
author = {Felfernig, Alexander and Reiterer, Stefan and Stettinger, Martin and Tiihonen, Juha},
doi = {10.1145/2701319.2701327},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/acm/felfernig2015a.pdf:pdf},
isbn = {9781450332736},
journal = {Proceedings of the Ninth International Workshop on Variability Modelling of Software-intensive Systems - VaMoS '15},
keywords = {Cognitive Aspects,Empirical Studies,Knowledge Engineering,Variability Modeling,verde},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {verde},
pages = {117--123},
title = {{Towards Understanding Cognitive Aspects of Configuration Knowledge Formalization}},
url = {http://dl.acm.org/citation.cfm?id=2701319.2701327},
year = {2015}
}
@article{Atzmueller2015a,
abstract = {Communities can intuitively be defined as subsets of nodes of a graph with a dense structure in the corresponding subgraph. However, for mining such communities usually only structural aspects are taken into account. Typically, no concise nor easily interpretable community description is provided. For tackling this issue, this paper focuses on description-oriented community detection using subgroup discovery. In order to provide both structurally valid and interpretable communities we utilize the graph structure as well as additional descriptive features of the graph's nodes. A descriptive community pattern built upon these features then describes and identifies a community, i.e., a set of nodes, and vice versa. Essentially, we mine patterns in the “description space” characterizing interesting sets of nodes (i.e., subgroups) in the “graph space”; the interestingness of a community is evaluated by a selectable quality measure. We aim at identifying communities according to standard community quality measures, while providing characteristic descriptions of these communities at the same time. For this task, we propose several optimistic estimates of standard community quality functions to be used for efficient pruning of the search space in an exhaustive branch-and-bound algorithm. We demonstrate our approach in an evaluation using five real-world data sets, obtained from three different social media applications.},
author = {Atzmueller, Martin and Doerfel, Stephan and Mitzlaff, Folke},
doi = {10.1016/j.ins.2015.05.008},
file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Atzmueller, Doerfel, Mitzlaff - 2015 - Description-oriented community detection using exhaustive subgroup discovery.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Community detection,Exceptional model mining,Exploratory data analysis,Optimistic estimates,Social network analysis,Subgroup discovery,purpura},
mendeley-groups = {con doi,informe},
mendeley-tags = {purpura},
month = {feb},
pages = {965--984},
title = {{Description-oriented community detection using exhaustive subgroup discovery}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025515003667 http://linkinghub.elsevier.com/retrieve/pii/S0020025515003667},
volume = {329},
year = {2016}
}
@article{Bakar2015a,
abstract = {Abstract Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners' adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners' guidelines were absent in the selected studies.},
author = {Bakar, Noor Hasrina and Kasirun, Zarinah M. and Salleh, Norsaremah},
doi = {10.1016/j.jss.2015.05.006},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/1-s2.0-S0164121215001004-main.pdf:pdf},
isbn = {01641212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Feature extractions,Natural language requirements,Requirements reuse,Software product lines,Systematic literature review,naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
month = {aug},
pages = {132--149},
publisher = {Elsevier Ltd.},
title = {{Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review}},
url = {http://dx.doi.org/10.1016/j.jss.2015.05.006 http://linkinghub.elsevier.com/retrieve/pii/S0164121215001004},
volume = {106},
year = {2015}
}
@book{Capilla2013,
address = {Berlin, Heidelberg},
author = {Capilla, Rafael and Bosch, J. and Kang, Kyo-Chul},
doi = {10.1007/978-3-642-36583-6},
editor = {Capilla, Rafael and Bosch, Jan and Kang, Kyo-Chul},
file = {:Users/josepplloo/Documents/Material de base para todos los cap{\`{i}}tulos/Algunos libros y documentos externos de apoyo/Systems and Software Variability Management-Concepts, tools and experiences.pdf:pdf},
isbn = {978-3-642-36582-9},
keywords = {verde},
mendeley-groups = {maerial base para el libro,con doi,propuesta,informe},
mendeley-tags = {verde},
pages = {317},
publisher = {Springer Berlin Heidelberg},
title = {{Systems and Software Variability Management}},
url = {http://link.springer.com/10.1007/978-3-642-36583-6},
year = {2013}
}
@article{Nickel2015,
abstract = {Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on tensor factorization methods and related latent variable models. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from theWeb. In particular, we discuss Google's Knowledge Vault project.},
author = {Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy},
doi = {10.1109/JPROC.2015.2483592},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/nickel2016.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {graph-based models,knowledge extraction,knowledge graphs,latent feature models,purpura,rojo,statistical relational},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {purpura,rojo},
month = {jan},
number = {1},
pages = {11--33},
title = {{A Review of Relational Machine Learning for Knowledge Graphs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7358050},
volume = {104},
year = {2016}
}
@article{Tucker2008,
abstract = {This paper addresses two important fundamental areas in product family formulation that have recently begun to receive great attention. First is the incorporation of market demand that we address through a data mining approach where realistic customer preference data are translated into performance design targets. Second is product architecture reconfiguration that we model as a dynamic design entity. The dynamic approach to product architecture optimization differs from conventional static approaches in that a product architecture is not fixed at the initial stage of product design, but rather evolves with fluctuations in customer performance preferences. The benefits of direct customer input in product family design will be realized through the cell phone product family example presented in this work. An optimal family of cell phones is created with modularity decisions made analytically at the engineering level that maximize company profit. Copyright {\textcopyright} 2008 by ASME.},
author = {Tucker, Conrad S. and Kim, Harrison M.},
doi = {10.1115/1.2838336},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/asme/041103{\_}1.pdf:pdf},
isbn = {1563478234},
issn = {10500472},
journal = {Journal of Mechanical Design},
keywords = {rojo},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {rojo},
number = {4},
pages = {041103},
title = {{Optimal Product Portfolio Formulation by Merging Predictive Data Mining With Multilevel Optimization}},
url = {http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?articleid=1449682},
volume = {130},
year = {2008}
}
@inproceedings{Woznica2012,
abstract = {A common problem with most of the feature selection meth- ods is that they often produce feature sets–models–that are not stable with respect to slight variations in the training data. Different authors tried to improve the feature selec- tion stability using ensemble methods which aggregate dif- ferent feature sets into a single model. However, the ex- isting ensemble feature selection methods suffer from two main shortcomings: (i) the aggregation treats the features independently and does not account for their interactions, and (ii) a single feature set is returned, nevertheless, in var- ious applications there might be more than one feature sets, potentially redundant, with similar information content. In this work we address these two limitations. We present a general framework in which we mine over different feature models produced from a given dataset in order to extract patterns over the models. We use these patterns to derive more complex feature model aggregation strategies that ac- count for feature interactions, and identify core and distinct feature models. We conduct an extensive experimental eval- uation of the proposed framework where we demonstrate its effectiveness over a number of high-dimensional problems from the fields of biology and text-mining.},
address = {New York, New York, USA},
author = {Woznica, Adam and Nguyen, Phong and Kalousis, Alexandros},
booktitle = {Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '12},
doi = {10.1145/2339530.2339674},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/IEE/woznica2012.pdf:pdf},
isbn = {9781450314626},
keywords = {all or part of,classification,data,feature selection,high-dimensional,model mining,naranja,or hard copies of,permission to make digital,purpura,stability,this work for},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja,purpura},
pages = {913},
publisher = {ACM Press},
title = {{Model mining for robust feature selection}},
url = {http://dl.acm.org/citation.cfm?doid=2339530.2339674},
year = {2012}
}
@article{Isaza2014,
abstract = {Prediction of process behavior is important and useful to understand the system status and to take early control actions during operation. This paper presents a fuzzy clustering approach for predicting situations (functional states) in complex process industries. The proposed methodology combines a static measurement, such as the result of a fuzzy classifier trained with historical process data, and an estimation algorithm based on Markov‘s theory for discrete event systems. The situation prediction function is integrated into a process monitoring system without increasing the computational cost, which makes real-time implementation feasible. The monitoring strategy includes two principal stages: an offline stage for designing the fuzzy classifier and the predictor, and an online stage for identifying current process situations and for estimating predicted functional states. Thus, at each sample time, the results of a fuzzy classifier are used as inputs in the prediction procedure. An attractive feature of our proposed method, for situation prediction, is that it provides information about the evolution of the process. The proposed approach was tested on a monitoring system for a power transmission line, and also for monitoring a boiler subsystem of a steam generator. Experimental results indicate that our proposed technique in this paper is effective and can be used as a tool, for operators, to be used in industrial process decision making.},
author = {Isaza, Claudia V. and Sarmiento, Henry O. and Kempowsky-Hamon, Tatiana and LeLann, Marie-Veronique},
doi = {10.1016/j.ins.2014.04.030},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/elsevier/Situation prediction based on fuzzy clustering for industrial processes 2014 Henry Sarmiento.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Complex process,Fuzzy clustering,Markov's chain,Situation prediction},
mendeley-groups = {estado del arte,con doi,informe},
month = {sep},
number = {7},
pages = {785--804},
publisher = {Elsevier Inc.},
title = {{Situation prediction based on fuzzy clustering for industrial complex processes}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025514004770 http://linkinghub.elsevier.com/retrieve/pii/S0020025514004770},
volume = {279},
year = {2014}
}
@incollection{PeterJ.Olver2013,
abstract = {Highly dynamic software systems are applications whose operations are particularly affected by changing requirements and uncertainty in their execution environments. Ideally such systems must evolve while they execute. To achieve this, highly dynamic software systems must be instrumented with self-adaptation mechanisms to monitor selected requirements and environment conditions to assess the need for evolution, plan desired changes, as well as validate and verify the resulting system. This chapter introduces fundamental concepts, methods, and techniques gleaned from self-adaptive systems engineering, as well as discusses their application to runtime evolution and their relationship with off-line software evolution theories. To illustrate the presented concepts, the chapter revisits a case study conducted as part of our research work, where self-adaptation techniques allow the engineering of a dynamic context monitoring infrastructure that is able to evolve at runtime. In other words, the monitoring infrastructure supports changes in monitoring requirements without requiring maintenance tasks performed manually by developers. The goal of this chapter is to introduce practitioners, researchers and students to the foundational elements of self-adaptive software, and their application to the continuos evolution of software systems at runtime.},
address = {Berlin, Heidelberg},
author = {Botterweck, Goetz and Pleuss, Andreas},
booktitle = {Evolving Software Systems},
doi = {10.1007/978-3-642-45398-4_9},
file = {:Users/josepplloo/Documents/primer inmersion/estado del arte/springer/10.1007@978-3-642-45398-49.pdf:pdf},
isbn = {978-3-642-45397-7},
keywords = {naranja},
mendeley-groups = {estado del arte,con doi,informe},
mendeley-tags = {naranja},
pages = {265--295},
publisher = {Springer Berlin Heidelberg},
title = {{Evolution of Software Product Lines}},
url = {http://link.springer.com/10.1007/978-3-642-45398-4{\_}9},
volume = {234},
year = {2014}
}
@article{Pokorny2015,
	abstract = {Now we have a number of database technologies called usually NoSQL, like key-value, column-oriented, and document stores as well as search engines and graph databases. Whereas SQL software vendors offer advanced products with the capability to handle highly complex queries and transactions, NoSQL databases share rather characteristics concerning scaling and performance, as e.g. auto-sharding, distributed query support, and integrated caching. Their drawbacks can be a lack of schema or data consistency, difficulty in testing and maintaining, and absence of a higher query language. Complex data modelling and the SQL language as the only access tool to data are missing here. On the other hand, last studies show that both SQL and NoSQL databases have value for both for transactional and analytical Big Data. Top databases providers offer rearchitected database technologies combining row data stores with columnar in-memory compression enabling processing large data sets and analytical querying, often over massive, continuous data streams. The technological progress led to development of massively parallel processing analytic databases. The paper presents some details of current database technologies, their pros and cons in different application environments, and emerging trends in this area.},
	author = {Pokorn{\'{y}}, Jaroslav},
	doi = {10.1145/2812428.2812429},
	file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Pokorn{\'{y}} - 2015 - Database technologies in the world of big data.pdf:pdf},
	isbn = {9781450333573},
	journal = {Proceedings of the 16th International Conference on Computer Systems and Technologies - CompSysTech '15},
	keywords = {big analytics,big data,data distribution,database technologies,newsql databases,nosql databases,transaction processing},
	mendeley-groups = {estado del arte},
	pages = {1--12},
	title = {{Database technologies in the world of big data}},
	url = {http://dl.acm.org/citation.cfm?doid=2812428.2812429},
	year = {2015}
}
@article{Castelluccia2014,
	author = {Castelluccia, Daniela and Boffoli, Nicola},
	doi = {10.1145/2579281.2579294},
	file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Castelluccia, Boffoli - 2014 - Service-oriented product lines.pdf:pdf},
	isbn = {978-3-642-36583-6},
	issn = {01635948},
	journal = {ACM SIGSOFT Software Engineering Notes},
	mendeley-groups = {estado del arte},
	number = {2},
	pages = {1--6},
	title = {{Service-oriented product lines}},
	url = {http://dl.acm.org/citation.cfm?doid=2579281.2579294},
	volume = {39},
	year = {2014}
}

@article{PerezLamancha2010,
	abstract = {This article presents a systematic review of the literature about Testing in Software Product Lines. The objective is to analyze the existing approaches to testing in software product lines, discussing the significant issues related to this area of knowledge and providing an up-to-date state of the art, which can serve as a basis for innovative research activities. The paper includes an analysis on how SPL research can contribute to dynamize the research in software testing.},
	author = {{P{\'{e}}rez Lamancha}, Beatriz and Polo, Macario and Piattini, Mario},
	doi = {10.1007/978-3-642-29578-2},
	file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/P{\'{e}}rez Lamancha, Polo, Piattini - 2010 - Systematic Review on Software Product Line Testing.pdf:pdf},
	isbn = {978-3-642-29577-5},
	issn = {18650929},
	journal = {5th International Conference, ICSOFT 2010, Athens, Greece, July 22-24, 2010. Revised Selected Papers},
	keywords = {Software product lines,Software testing,Survey,Systematic review,Test},
	mendeley-groups = {estado del arte},
	pages = {pp. 58--71},
	title = {{Systematic Review on Software Product Line Testing}},
	url = {http://www.springerlink.com/index/10.1007/978-3-642-29578-2},
	volume = {170},
	year = {2010}
}

@article{Mangalova2014,
	abstract = {The paper deals with a modeling procedure which aims to predict the power output of wind farm electricity generators. The following modeling steps are proposed: factor selection, raw data pretreatment, model evaluation and optimization. Both heuristic and formal methods are combined to construct the model. The basic modeling approach here is the k-nearest neighbors method.},
	author = {Mangalova, E. and Agafonov, E.},
	doi = {10.1016/j.ijforecast.2013.07.008},
	file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Mangalova, Agafonov - 2014 - Wind power forecasting using the mmlmath altimg=si14.gif display=inline overflow=scroll xmlnsxocs=httpwww.e.pdf:pdf},
	issn = {01692070},
	journal = {International Journal of Forecasting},
	keywords = {Cross-validation,Data mining,Energy forecasting*,Feature selection,Forecasting competitions*,Nonparametric models,Regression tree},
	mendeley-groups = {estado del arte},
	month = {apr},
	number = {2},
	pages = {402--406},
	title = {{Wind power forecasting using the {\textless}mml:math altimg="si14.gif" display="inline" overflow="scroll" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xs="http://www.w3.org/2001/XMLSchema" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http}},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207013000848},
	volume = {30},
	year = {2014}
}

@article{FAMILIAR, author = {Mathieu Acher and Philippe Collet and Philippe Lahire and Robert B. France}, title = {FAMILIAR: A domain-specific language for large scale management of feature models}, journal = {Science of Computer Programming (SCP)}, volume = {78}, number = {6}, year = {2013}, pages = {657-681}, ee = {http://dx.doi.org/10.1016/j.scico.2012.12.004}, }

@inproceedings{Mendonca:2009:SSP:1639950.1640002,
	author = {Mendonca, Marcilio and Branco, Moises and Cowan, Donald},
	title = {S.P.L.O.T.: Software Product Lines Online Tools},
	booktitle = {Proceedings of the 24th ACM SIGPLAN Conference Companion on Object Oriented Programming Systems Languages and Applications},
	series = {OOPSLA '09},
	year = {2009},
	isbn = {978-1-60558-768-4},
	location = {Orlando, Florida, USA},
	pages = {761--762},
	numpages = {2},
	url = {http://doi.acm.org/10.1145/1639950.1640002},
	doi = {10.1145/1639950.1640002},
	acmid = {1640002},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {automated reasoning, feature model automated analysis, feature models, interactive configuration, software product lines},
} 
@inproceedings{Kalagnanam2004,
	address = {New York, New York, USA},
	author = {Kalagnanam, Jayant and Singh, Moninder and Verma, Sudhir and Patek, Michael and Wong, Yuk Wah},
	booktitle = {Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '04},
	doi = {10.1145/1014052.1016918},
	file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Kalagnanam et al. - 2004 - A system for automated mapping of bill-of-materials part numbers.pdf:pdf},
	isbn = {1581138889},
	mendeley-groups = {maerial base para el libro},
	pages = {805},
	publisher = {ACM Press},
	title = {{A system for automated mapping of bill-of-materials part numbers}},
	url = {http://portal.acm.org/citation.cfm?doid=1014052.1016918},
	year = {2004}
}
@article{Harding2006,
	abstract = {The paper reviews applications of data mining in manufacturing engineering, in particu- lar production processes, operations, fault detection, maintenance, decision support, and product quality improvement. Customer relationship management, information integra- tion aspects, and standardization are also briefly discussed. This review is focused on demonstrating the relevancy of data mining to manufacturing industry, rather than dis- cussing the data mining domain in general. The volume of general data mining literature makes it difficult to gain a precise view of a target area such as manufacturing engineer- ing, which has its own particular needs and requirements for mining applications. This review reveals progressive applications in addition to existing gaps and less considered areas such as manufacturing planning and shop floor control.},
	author = {Harding, J. a. and Shahbaz, M. and Srinivas and Kusiak, a.},
	doi = {10.1115/1.2194554},
	file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Harding et al. - 2006 - Data Mining in Manufacturing A Review.pdf:pdf},
	issn = {10871357},
	journal = {Journal of Manufacturing Science and Engineering},
	mendeley-groups = {estado del arte},
	number = {4},
	pages = {969},
	title = {{Data Mining in Manufacturing: A Review}},
	volume = {128},
	year = {2006}
}
@article{Wang1998,
	abstract = {In process plant operation and control, modern distributed control and automatic data logging systems create large volumes of data that contain valuable information about normal and abnormal operations, significant disturbances, and changes in operational and control strategies. These data have tended to be underexploited for a variety of reasons, including the large volume and lack of effective automatic computer-based support tools. This paper considers a data mining system that is able to automatically cluster the data into classes corresponding to various operational modes and thereby provide some structure for analysis of behavioral responses. The method is illustrated by reference to a case study of a refinery fluid catalytic cracking process.},
	author = {Wang, X Z and McGreavy, C},
	doi = {10.1021/ie970620h},
	file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Wang, McGreavy - 1998 - Automatic Classification for Mining Process Operational Data.pdf:pdf},
	isbn = {0888-5885},
	issn = {0888-5885},
	journal = {Industrial {\&} Engineering Chemistry Research},
	mendeley-groups = {estado del arte,propuesta},
	month = {jun},
	number = {6},
	pages = {2215--2222},
	title = {{Automatic Classification for Mining Process Operational Data}},
	url = {http://dx.doi.org/10.1021/ie970620h http://pubs.acs.org/doi/abs/10.1021/ie970620h},
	volume = {37},
	year = {1998}
}
@article{Pedram2015,
	abstract = {The software has become a modern asset and competitive product. The product line that has long been used in manufacturing and construction industries nowadays has attracted a lot of attention in software industry. Most importance of product line engineering approach is in cost and time issues involved in marketing. Feature model is one of the most important methods of documenting variability in product line that shows product features and their dependencies. Because of the magnitude and complexity of the product line, build and maintain feature models are complex and time-consuming work. In this article feature model importance and position in product line is discussed and feature model extraction methods are reviewed and compared.},
	author = {Pedram, Saba and Mohsenzadeh, Mehran and Ahrabi, Amir Azimi Alasti},
	file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Pedram, Mohsenzadeh, Ahrabi - 2015 - A Review of Feature Model Position in the Software Product Line and Its Extraction Methods.pdf:pdf},
	journal = {International Journal of Computer Science and Security (ICSS)},
	keywords = {extraction method review,feature model,software product line},
	mendeley-groups = {estado del arte},
	number = {5},
	pages = {274--279},
	title = {{A Review of Feature Model Position in the Software Product Line and Its Extraction Methods}},
	volume = {9},
	year = {2015}
}
@book{Kang2010,
	author = {Kang, Kc and Sugumaran, V and Park, S},
	file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Kang, Sugumaran, Park - 2010 - Applied software product line engineering.pdf:pdf},
	isbn = {9781420068429},
	mendeley-groups = {maerial base para el libro},
	pages = {561},
	title = {{Applied software product line engineering}},
	url = {http://books.google.com.mx/books/about/Applied{\_}Software{\_}Product{\_}Line{\_}Engineerin.html?id=7XO{\_}ghvkpt4C{\&}redir{\_}esc=y},
	year = {2010}
}

@book{Rashid2011,
	abstract = {Software product lines provide a systematic means of managing variability in a suite of products. They have many benefits but there are three major barriers that can prevent them from reaching their full potential. First, there is the challenge of scale: a large number of variants may exist in a product line context and the number of interrelationships and dependencies can rise exponentially. Second, variations tend to be systemic by nature in that they affect the whole architecture of the software product line. Third, software product lines often serve different business contexts, each with its own intricacies and complexities. The AMPLE (http://www.ample-project.net/) approach tackles these three challenges by combining advances in aspect-oriented software development and model-driven engineering. The full suite of methods and tools that constitute this approach are discussed in detail in this edited volume and illustrated using three real-world industrial case studies. {\textcopyright} Cambridge University Press 2011.},
	author = {Rashid, A and Royer, J.-C. and Rummler, A},
	booktitle = {Aspect-Oriented, Model-Driven Software Product Lines: The AMPLE Way},
	doi = {10.1017/CBO9781139003629},
	file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Rashid, Royer, Rummler - 2011 - Aspect-oriented, model-driven software product lines The AMPLE way.pdf:pdf},
	isbn = {9780521767224},
	mendeley-groups = {estado del arte},
	pages = {1--464},
	title = {{Aspect-oriented, model-driven software product lines: The AMPLE way}},
	url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84923405380{\&}partnerID=40{\&}md5=f8ae074bdfd181d3b7889ce2bfd266bd},
	year = {2011}
}
@article{Hornick2009,
	author = {Hornick, Mark F. and Marcad{\'{e}}, Erik and Venkayala, Sunil},
	doi = {10.1016/B978-0-7020-2797-0.00001-1},
	file = {:Users/josepplloo/Library/Application Support/Mendeley Desktop/Downloaded/Hornick, Marcad{\'{e}}, Venkayala - 2009 - Chapter 1.pdf:pdf},
	isbn = {1111001111},
	issn = {15710661},
	journal = {Java Data Mining},
	mendeley-groups = {estado del arte},
	pages = {1--4},
	pmid = {20314319},
	title = {{Chapter 1}},
	volume = {1},
	year = {2009}
}
